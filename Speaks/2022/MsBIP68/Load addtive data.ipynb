{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Load additive data from Data Lake\n",
        "\n",
        "Get raw data from Data Lake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-06T13:03:54.243598Z",
              "execution_start_time": "2022-02-06T13:03:53.8297217Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-06T13:03:53.3960225Z",
              "session_id": 4,
              "session_start_time": null,
              "spark_pool": "SmallSparkpool",
              "state": "finished",
              "statement_id": 141
            },
            "text/plain": [
              "StatementMeta(SmallSparkpool, 4, 141, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#objectName = dbutils.widgets.get(\"Tablename\")\n",
        "objectName = 'Booking'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "collapsed": false,
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-06T12:29:27.9010761Z",
              "execution_start_time": "2022-02-06T12:29:25.43851Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-06T12:29:24.1080889Z",
              "session_id": 4,
              "session_start_time": null,
              "spark_pool": "SmallSparkpool",
              "state": "finished",
              "statement_id": 114
            },
            "text/plain": [
              "StatementMeta(SmallSparkpool, 4, 114, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "df = (spark\n",
        "    .read\n",
        "    .format('csv') \n",
        "    .option(\"delimiter\", \";\")\n",
        "    .option(\"multiline\", True)\n",
        "    .option(\"quote\", \"\\\"\")\n",
        "    .option(\"escape\", \"\\\"\")\n",
        "    .option(\"header\",True)\n",
        "    .option('path',f'abfss://<your own filesystem>@<your own filesystem>.dfs.core.windows.net/{objectName}')\n",
        "    .load()\n",
        "    #.limit(1)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Handle schema\n",
        "Remove training header names from dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-06T12:29:30.1502098Z",
              "execution_start_time": "2022-02-06T12:29:29.5959524Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-06T12:29:29.232158Z",
              "session_id": 4,
              "session_start_time": null,
              "spark_pool": "SmallSparkpool",
              "state": "finished",
              "statement_id": 115
            },
            "text/plain": [
              "StatementMeta(SmallSparkpool, 4, 115, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df = df.select([col(c).alias(c[:c.index(' ')]) for c in df.columns])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Remove columns that are not used - if they exist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-06T12:47:03.6641166Z",
              "execution_start_time": "2022-02-06T12:47:03.2466771Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-06T12:47:02.8436358Z",
              "session_id": 4,
              "session_start_time": null,
              "spark_pool": "SmallSparkpool",
              "state": "finished",
              "statement_id": 135
            },
            "text/plain": [
              "StatementMeta(SmallSparkpool, 4, 135, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ColumnnamesToDelete = spark.createDataFrame(\n",
        "    [\n",
        "        (1, \"booking_description\"),\n",
        "        (2, \"last_change_date\"),\n",
        "        (3, \"color_code_fk\")\n",
        "    ]\n",
        "    ,[\"number\",\"columnname\"]\n",
        "    )\n",
        "\n",
        "for d in ColumnnamesToDelete.collect():\n",
        "    if d[\"columnname\"] in df.columns:\n",
        "        df = df.drop(d[\"columnname\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Change column datatypes if they exits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StringType,BooleanType,DateType\n",
        "\n",
        "if 'date' in df.columns:\n",
        "    df = df.withColumn(\"date\", df[\"date\"].cast(DateType()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Remove characters not used in data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import regexp_replace\n",
        "\n",
        "SearchInColumns = spark.createDataFrame(\n",
        "    [\n",
        "        (1, \"start_time\"),\n",
        "        (2, \"end_time\")\n",
        "    ]\n",
        "    ,[\"number\",\"columnname\"]\n",
        "    )\n",
        "\n",
        "CharacterChanges = spark.createDataFrame(\n",
        "    [\n",
        "        (1, \"\\\\+\", \"\")\n",
        "    ]\n",
        "    ,[\"number\",\"RegExString\",\"NewString\"]\n",
        "    )\n",
        "\n",
        "for s in SearchInColumns.collect():\n",
        "    if s[\"columnname\"] in df.columns:\n",
        "        for r in CharacterChanges.collect():\n",
        "            df = df.withColumn(s[\"columnname\"], regexp_replace(col(s[\"columnname\"]), r[\"RegExString\"], r[\"NewString\"]))\n",
        "\n",
        "#display(df.limit(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#display(df.limit(10))\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Write data to Bronze area\n",
        "\n",
        "Write new data to the delta lake in Bronze version<br>\n",
        "Notice the APPEND part of the 4th line - this appends all data to the existing data (delta lake)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-06T12:51:53.670164Z",
              "execution_start_time": "2022-02-06T12:51:49.627727Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-06T12:51:49.1380262Z",
              "session_id": 4,
              "session_start_time": null,
              "spark_pool": "SmallSparkpool",
              "state": "finished",
              "statement_id": 139
            },
            "text/plain": [
              "StatementMeta(SmallSparkpool, 4, 139, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "bronze_loc = f'abfss://<your own filesystem>@<your own filesystem>.dfs.core.windows.net/Bronze/{objectName}'\n",
        "\n",
        "df.write.mode(\"append\").format(\"delta\").save(bronze_loc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Write data to Silver area\n",
        "\n",
        "Load to Silver data lake with correct partition approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-06T12:53:52.1528369Z",
              "execution_start_time": "2022-02-06T12:53:48.0522851Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-06T12:53:47.6428477Z",
              "session_id": 4,
              "session_start_time": null,
              "spark_pool": "SmallSparkpool",
              "state": "finished",
              "statement_id": 140
            },
            "text/plain": [
              "StatementMeta(SmallSparkpool, 4, 140, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "silver_loc = f'abfss://<your own filesystem>@<your own filesystem>.dfs.core.windows.net/Silver/{objectName}'\n",
        "\n",
        "parquetFile = spark.read.parquet(bronze_loc)\n",
        "\n",
        "parquetFile.repartition(1).write.parquet(silver_loc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Things and notes to keep for later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-01T12:47:08.6035854Z",
              "execution_start_time": "2022-02-01T12:46:46.0449079Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-01T12:46:45.6143404Z",
              "session_id": 1,
              "session_start_time": null,
              "spark_pool": "SmallSparkpool",
              "state": "finished",
              "statement_id": 55
            },
            "text/plain": [
              "StatementMeta(SmallSparkpool, 1, 55, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## managed table - hard link between objects\n",
        "## df2.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Bookings\")\n",
        "\n",
        "## unmanaged table - soft link between objects\n",
        "## df2.write.mode(\"overwrite\").format(\"delta\").option(\"path\",save_loc).saveAsTable(\"Bookings\")\n",
        "\n",
        "## statement = 'select * from Bookings'\n",
        "\n",
        "## spark.sql(statement).createOrReplaceTempView(\"Temp_Bookings\")\n"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

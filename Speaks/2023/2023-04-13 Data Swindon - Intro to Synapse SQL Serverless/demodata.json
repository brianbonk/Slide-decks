[
    {
        "categories": [
            "Synapse"
        ],
        "content": "Yes, you read it correctly. You can travel in time on your data using Azure Data Bricks and DeltaLake. The feature is quite easy to use and comes with a lot of very nice features and business value. Even a value so powerfull that the business does not need to make descissions up front when you are building your data warehouse.\nA little word about the background Azure has abopted DataBricks into the Azure Synapse portfolio with seemless integration between all the services from within the umbrella of Azure Synapse.\nNotebooks directly in Synapse can do a lot of the stuff from DataBricks and are almost on par with the features between the two. But on some areas the DataBricks engine is more powerful than the build in Notebooks in Synapse.\nThe reason, at least some of it, is that Microsoft needs to do some rewriting of the features to fit into the services between the Blob Storage and the Notebooks. Some of these things are also a part of the close collaboration between Microsoft and DataBricks, and Microsoft does not want to out-smart DataBricks on everything.\nIf you would like to learn more about the cooperation between DataBricks and Microsoft - you can read this external link\nLets code The followind code blocks are creating a dataframe to work with. I\u0026rsquo;ll go through the steps below the picture.\nThe first section is importing the functions library from pyspark.sql - we need that to create the dataframe in a SQL context.\nThen I\u0026rsquo;m creating some dummy data in a dataframe named data1. It looks just like a CTE on SQL and the syntax is also very much like so.\nIn the next section I\u0026rsquo;m displaying the dataframe to test if the data looks right.\nThe last section writes this dataframe to a delta-table with the name of \u0026ldquo;climate\u0026rdquo;.\nAbove section is investigating the table \u0026ldquo;climate\u0026rdquo; by a nifty %sql language change and then executing the DESCRIBE function on the table.\nThis gives an output as below:\nsubset of the table\n   version timestamp userName operation operationParameters     0 date T15:56:09.000+0000 username CREATE TABLE AS SELECT Map(isManaged -\u0026gt; true, description -\u0026gt; null, partitionBy -\u0026gt; [], properties -\u0026gt; {})    From the column OPERATION we can read that the table has been created and from the TIMESTAMP and USERNAME we can read the info on when and who.\nThe important part here, is to know that the VERSION starts with 0 (zero).\nThen I\u0026rsquo;m adding another two rows to the dataset by executing the below sections.\nThe first section is creating a new dataframe with the name of data2 and adding the two rows of data.\nThe next section is adding (the append statement) data to the existing table \u0026ldquo;climate\u0026rdquo;.\nAgain I\u0026rsquo;m accessing the data to see if everything is ok, now with 6 rows of data.\nThe last section is a describe again - and now we see that the meta-data and history of the table has been updated.\nsubset of the table\n   version timestamp userName operation operationParameters     1 date T15:57:35.000+0000 username WRITE Map(mode -\u0026gt; Append, partitionBy -\u0026gt; [])   0 date T15:56:09.000+0000 username CREATE TABLE AS SELECT Map(isManaged -\u0026gt; true, description -\u0026gt; null, partitionBy -\u0026gt; [], properties -\u0026gt; {})    We see that the new entry with VERSION = 1, now has an OPERATION of WRITE and the OPERATION METRICS shows us two file (which in this example is equal to the added rows).\nThe timetravel Now to the fun part. We can do timetravel on the created dataset.\nWith below statement get the data as it looked like in VERSION 0:\n%sql select * from climate VERSION AS OF 0    id hour temperature wind     111 12 25 8   112 13 24 11   113 14 23 9   114 15 24 12    And below gives me the data as it looks after VERSION 1 has been implemented (the total dataset)\n%sql select * from climate VERSION AS OF 1    id hour temperature wind     111 12 25 8   112 13 24 11   113 14 23 9   114 15 24 12   115 16 18 15   116 17 17 23    If I want to achieve the same in clean python I can execute below statement:\ndf = spark.read.format(\u0026#34;delta\u0026#34;).option(\u0026#34;versionAsOf\u0026#34;, 1).load(\u0026#34;dbfs:/user/hive/warehouse/climate\u0026#34;) df.show() I can also do the same with timestamps to get data from a specific time using below statement:\n%sql select * from climate TIMESTAMP AS OF \u0026#39;\u0026lt;date\u0026gt;\u0026#39; Try it out yourself I\u0026rsquo;ve made a small demo script of the above excamples which you can find from my GitHub public repo here and try out on your own.\nIf you would like to read more about the feature and the syntax to use it - you can refer to below external links:\n Databricks.com Microsoft docs Complete reference of Delta Lake  ",
        "permalink": "https://brianbonk.dk/blog/timetravel-in-databricks/",
        "tags": [
            "DataBricks"
        ],
        "title": "Timetravel in DataBricks"
    },
    {
        "categories": [
            "Power BI"
        ],
        "content": "edit: Added the same aproach for JSON at the bottom of this post\nHave you also seen alot of \u0026ldquo;Helper Queries\u0026rdquo; in Power Query when working with files from folders? I think it is very cluttered to have all these helpers laying around in the Power Query editor.\nWhat I\u0026rsquo;m used to see Above is the usual way of Power Query to handle several files in the same folder. The approach is done to have only 1 (usually the first) file to create the schema from. This is also quite ok for at single report using only one or a few folders as sources.\nBut when you start to have a lot of folders and fro this, a lot of helper queries, the approach gets all cluttered and almost unable to read when making changes and debugging.\nThe cure for CSV I\u0026rsquo;ve been frustrated with this for a while now, and finally sat down to create the Power Query to handle this.\nIt is like below:\nlet Source = AzureStorage.Blobs(\u0026quot;https://\u0026lt;blobstore\u0026gt;.blob.core.windows.net/\u0026lt;folder\u0026gt;\u0026quot;), Dataarea = Table.SelectRows(Source, each Text.StartsWith([Name], \u0026quot;\u0026lt;starttext\u0026gt;\u0026quot;)), LatestData = Table.SelectRows(Dataarea, let latest = List.Max(Dataarea[Date modified]) in each [Date modified] = latest), AddColumnData = Table.AddColumn(LatestData, \u0026quot;CsvContent\u0026quot;, each Csv.Document([Content], [Delimiter=\u0026quot;;\u0026quot;])), KeepDataColumn = Table.SelectColumns(AddColumnData,{\u0026quot;CsvContent\u0026quot;}), ExpandTable = Table.ExpandTableColumn(KeepDataColumn, \u0026quot;CsvContent\u0026quot;, Table.ColumnNames(KeepDataColumn[CsvContent]{0}), Table.ColumnNames(KeepDataColumn[CsvContent]{0})), AddHeaders = Table.PromoteHeaders(ExpandTable, [PromoteAllScalars=true]), RenameHeaders = Table.RenameColumns( AddHeaders, List.Zip( { Table.ColumnNames (AddHeaders), List.Transform(Table.ColumnNames (AddHeaders), each Text.BeforeDelimiter(_, \u0026quot; \u0026quot;) ) } ) ) in RenameHeaders I\u0026rsquo;ll go through each line/segment below and my approach to the solution.\nSource This is quite standard - the source to a blob storage (or any other folder source). Remember to autenticate to the storage account before continuing.\nDataarea This line selects only the files with the name starting with the text in . If you need it, you can change this to another approach. If you need to select all files in the folder, you can skip this line and go on the next. Remember to change the source-element in the next line.\nLatestData This line makes sure to select only the lastest file (based on Modified Date from Blob storage). This in order to help if you have several full-loads in the source and only needs the latest file on the query. If you need all the files in the query, you can skip this also.\nAddColumnData Here I add a new column, based on the default [content]-column. The specific csv-file has semi-colon as column-delimiter. Change this to fit your needs.\nKeppDataColumn To clean up the UI and datastorage within the Power BI file, I only keep the newly created column, that contains the data I need.\nExpandTable Here the data-column is exspanded with default columnnames.\nAddHeaders The headers are now added with promoting the first line to a header line.\nRenameHeaders In this specific scenario, the file has column names with trailing datatypes (e.g. \u0026ldquo;Name\u0026rdquo; NVARCHAR(255)). Here I clean the column based on the first whitespace in the header and remove everything after this.\nThe backside (if it is a backside) With above approach you need to specify your datatypes manually. I can\u0026rsquo;t make up with my self, if this is a good or a bad thing. The bad thing is that it is not done automatically by Power BI (with what follows of possible errors). The good thing is, that I am forced to make a descision about the data types for each column.\nI think I like \u0026ldquo;the good\u0026rdquo; thing.\nThe cure for JSON let Source = AzureStorage.Blobs(\u0026quot;https://\u0026lt;blobstore\u0026gt;.blob.core.windows.net/\u0026lt;folder\u0026gt;\u0026quot;), Dataarea = Table.SelectRows(Source, each Text.StartsWith([Name], \u0026quot;\u0026lt;starttext\u0026gt;\u0026quot;)), AddColumnData = Table.AddColumn(Dataarea, \u0026quot;JsonContent\u0026quot;, each Json.Document([Content])), KeepDataColumn = Table.SelectColumns(AddColumnData, {\u0026quot;JsonContent\u0026quot;}), FieldsToExpand = List.Distinct( List.Combine(List.Transform(KeepDataColumn[JsonContent]{0}, Record.FieldNames)) ), InitialTable = Table.FromRecords(KeepDataColumn[JsonContent]{0}, FieldsToExpand, MissingField.UseNull) in InitialTable Happy coding 😊.\n",
        "permalink": "https://brianbonk.dk/blog/get-rid-of-helper-queries-in-power-query/",
        "tags": [
            "M (power query)"
        ],
        "title": "Get rid of Helper Queries in Power Query"
    },
    {
        "categories": [
            "SQL Server"
        ],
        "content": "Ever been as frustrated as I have when importing flat files to a SQL Server and the format suddenly changes in production?\nCommonly used integration tools (like SSIS) are very dependent on the correct, consistent and same metadata when working with flat files.\nSo I’ve come up with an alternative solution that I would like to share with you.\nWhen implemented, the process of importing flat files with changing metadata is handled in a structured, and most important, resiliant way. Even if the columns change order or existing columns are missing.\nBackground When importing flat files to SQL server almost every standard integration tool (including TSQL bulkload) requires fixed metadata from the files in order to work with them.\nThis is quite understandable, as the process of data transportation from the source to the destination needs to know where to map every column from the source to the defined destination.\nLet me make an example:\nA source flat file table like below needs to be imported to a SQL server database. This file could be imported to a SQL Server database (in this example named FlatFileImport) with below script:\ncreate table dbo.personlist ( [name] varchar(20), [gender] varchar(10), [age] int, [city] varchar(20), [country] varchar(20) ); BULK INSERT dbo.personlist FROM \u0026#39;c:\\source\\personlist.csv\u0026#39; WITH ( FIRSTROW = 2, FIELDTERMINATOR = \u0026#39;;\u0026#39;, --CSV field delimiter \tROWTERMINATOR = \u0026#39;\\n\u0026#39;, --Use to shift the control to next row \tTABLOCK, CODEPAGE = \u0026#39;ACP\u0026#39; ); select * from dbo.personlist; The result: If the column ‘Country’ would be removed from the file after the import has been setup, the process of importing the file would either break or be wrong (depending on the tool used to import the file) The metadata of the file has changed.\n-- import data from file with missing column (Country) truncate table dbo.personlist; BULK INSERT dbo.personlist FROM \u0026#39;c:\\source\\personlistmissingcolumn.csv\u0026#39; WITH ( FIRSTROW = 2, FIELDTERMINATOR = \u0026#39;;\u0026#39;, --CSV field delimiter \tROWTERMINATOR = \u0026#39;\\n\u0026#39;, --Use to shift the control to next row \tTABLOCK, CODEPAGE = \u0026#39;ACP\u0026#39; ); select * from dbo.personlist; With this example, the import seems to go well, but upon browsing the data, you’ll see that only one row is imported and the data is wrong.\nThe same would happen if the columns ‘Gender’ and ‘Age’ where to switch places. Maybe the import would not break, but the mapping of the columns to the destination would be wrong, as the ‘Age’ column would go to the ‘Gender’ column in the destination and vice versa. This due to the order and datatype of the columns. If the columns had the same datatype and data could fit in the columns, the import would go fine – but the data would still be wrong.\n-- import data from file with switched columns (Age and Gender) truncate table dbo.personlist; BULK INSERT dbo.personlist FROM \u0026#39;c:\\source\\personlistswitchedcolumns.csv\u0026#39; WITH ( FIRSTROW = 2, FIELDTERMINATOR = \u0026#39;;\u0026#39;, --CSV field delimiter \tROWTERMINATOR = \u0026#39;\\n\u0026#39;, --Use to shift the control to next row \tTABLOCK, CODEPAGE = \u0026#39;ACP\u0026#39; ); When importing the same file, but this time with an extra column (Married) – the result would also be wrong:\n-- import data from file with new extra column (Married) truncate table dbo.personlist; BULK INSERT dbo.personlist FROM \u0026#39;c:\\source\\personlistextracolumn.csv\u0026#39; WITH ( FIRSTROW = 2, FIELDTERMINATOR = \u0026#39;;\u0026#39;, --CSV field delimiter  ROWTERMINATOR = \u0026#39;\\n\u0026#39;, --Use to shift the control to next row  TABLOCK, CODEPAGE = \u0026#39;ACP\u0026#39; ); select * from dbo.personlist; The result: The above examples are made with pure TSQL code. If it was to be made with an integration tool like SQL Server Integration Services, the errors would be different and the SSIS package would throw more errors and not be able to execute the data transfer.\nThe cure When using the above BULK INSERT functionality from TSQL the import process often goes well, but the data is wrong with the source file is changed.\nThere is another way to import flat files. This is using the OPENROWSET functionality from TSQL.\nIn section E of the example scripts from MSDN, it is described how to use a format file. A format file is a simple XML file that contains information of the source files structure – including columns, datatypes, row terminator and collation.\nGeneration of the initial format file for a curtain source is rather easy when setting up the import.\nBut what if the generation of the format file could be done automatically and the import process would be more streamlined and manageable – even if the structure of the source file changes?\nFrom my GitHub project you can download a home brewed .NET console application that solves just that.\nIf you are unsure of the .EXE files content and origin, you can download the code and build your own version of the GenerateFormatFile.exe application.\nAnother note is that I’m not hard core .Net developer, so someone might have another way of doing this. You are very welcome to contribute to the GitHub project in that case.\nThe application demands inputs as below: Example usage:\ngenerateformatfile.exe -p c:\\source\\ -f personlist.csv -o personlistformatfile.xml -d ;\nThe above script generates a format file in the directory c:\\source\\ and names it personlistFormatFile.xml.\nThe content of the format file is as follows: The console application can also be called from TSQL like this:\n-- generate format file declare @cmdshell varchar(8000); set @cmdshell = \u0026#39;c:\\source\\generateformatfile.exe -p c:\\source\\ -f personlist.csv -o personlistformatfile.xml -d ;\u0026#39; exec xp_cmdshell @cmdshell; If by any chance the xp_cmdshell feature is not enabled on your local machine – then please refer to this post from Microsoft: Enable xp_cmdshell\nUsing the format file After generation of the format file, it can be used in TSQL script with OPENROWSET.\nExample script for importing the ‘personlist.csv’\n-- import file using format file select * into dbo.personlist_bulk from openrowset( bulk \u0026#39;c:\\source\\personlist.csv\u0026#39;, formatfile=\u0026#39;c:\\source\\personlistformatfile.xml\u0026#39;, firstrow=2 ) as t; select * from dbo.personlist_bulk; This loads the data from the source file to a new table called ‘personlist_bulk’.\nFrom here the load from ‘personlist_bulk’ to ‘personlist’ is straight forward:\n-- load data from personlist_bulk to personlist truncate table dbo.personlist; insert into dbo.personlist (name, gender, age, city, country) select * from dbo.personlist_bulk; select * from dbo.personlist; drop table dbo.personlist_bulk; Load data even if source changes The above approach works if the source is the same every time it loads. But with a dynamic approach to the load from the bulk table to the destination table it can be assured that it works even if the source table is changed in both width (number of columns) and column order.\nFor some the script might seem cryptic – but it is only a matter of generating a list of column names from the source table that corresponds with the column names in the destination table.\n-- import file with different structure -- generate format file if exists(select OBJECT_ID(\u0026#39;personlist_bulk\u0026#39;)) drop table dbo.personlist_bulk declare @cmdshell varchar(8000); set @cmdshell = \u0026#39;c:\\source\\generateformatfile.exe -p c:\\source\\ -f personlistmissingcolumn.csv -o personlistmissingcolumnformatfile.xml -d ;\u0026#39; exec xp_cmdshell @cmdshell; -- import file using format file select * into dbo.personlist_bulk from openrowset( bulk \u0026#39;c:\\source\\personlistmissingcolumn.csv\u0026#39;, formatfile=\u0026#39;c:\\source\\personlistmissingcolumnformatfile.xml\u0026#39;, firstrow=2 ) as t; -- dynamic load data from bulk to destination declare @fieldlist varchar(8000); declare @sql nvarchar(4000); select @fieldlist = stuff((select \u0026#39;,\u0026#39; + QUOTENAME(r.column_name) from ( select column_name from INFORMATION_SCHEMA.COLUMNS where TABLE_NAME = \u0026#39;personlist\u0026#39; ) r join ( select column_name from INFORMATION_SCHEMA.COLUMNS where TABLE_NAME = \u0026#39;personlist_bulk\u0026#39; ) b on b.COLUMN_NAME = r.COLUMN_NAME for xml path(\u0026#39;\u0026#39;)),1,1,\u0026#39;\u0026#39;); print (@fieldlist); set @sql = \u0026#39;truncate table dbo.personlist;\u0026#39; + CHAR(10); set @sql = @sql + \u0026#39;insert into dbo.personlist (\u0026#39; + @fieldlist + \u0026#39;)\u0026#39; + CHAR(10); set @sql = @sql + \u0026#39;select \u0026#39; + @fieldlist + \u0026#39; from dbo.personlist_bulk;\u0026#39;; print (@sql) exec sp_executesql @sql The result is a TSQL statement what looks like this:\ntruncate table dbo.personlist; insert into dbo.personlist ([age],[city],[gender],[name]) select [age],[city],[gender],[name] from dbo.personlist_bulk; The exact same thing would be able to be used with the other source files in this demo. The result is that the destination table is correct and loaded with the right data every time – and only with the data that corresponds with the source. No errors will be thrown.\nFrom here there are some remarks to be taken into account:\nAs no errors are thrown, the source files could be empty and the data updated could be blank in the destination table. This is to be handled by processed outside this demo. Further work As this demo and post shows it is possible to handle dynamic changing flat source files. Changing columns, column order and other changes, can be handled in an easy way with a few lines of code.\nGoing from here, a suggestion could be to set up processes that compared the two tables (bulk and destination) and throws an error if X amount of the columns are not present in the bulk table or X amount of columns are new.\nIt is also possible to auto generate missing columns in the destination table based on columns from the bulk table.\nThe only boundaries are set by limits to your imagination\nSummary With this blogpost I hope to have given you inspiration to build your own import structure of flat files in those cases where the structure might change.\nAs seen above the approach needs some .NET programming skills – but when it is done and the console application has been built, it is simply a matter of reusing the same application around the different integration solutions in your environment.\nHappy coding 🙂\n",
        "permalink": "https://brianbonk.dk/blog/how-to-import-flat-files-with-a-varying-number-of-columns-in-sql-server/",
        "tags": [
            "Flat files"
        ],
        "title": "How to Import Flat Files With a Varying Number of Columns in SQL Server"
    },
    {
        "categories": [
            "SQL Server"
        ],
        "content": "Have you ever tried to delete an object from the database by mistake or other error?\nThen you should read on in this short post.\nI recently came across a good co-worker of mine who lost one of the views on the developer database. He called me for help.\nFortunately the database was in FULL RECOVERY mode – so I could extract the object from the database log and send the script to him for his further work that day. I think I saved him a whole day of work…\nHere is the script I used:\nselect convert(varchar(max),substring([RowLog Contents 0], 33, LEN([RowLog Contents 0]))) as [Script] from fn_dblog(NULL,NULL) where 1=1 and [Operation]=\u0026#39;LOP_DELETE_ROWS\u0026#39; and [Context]=\u0026#39;LCX_MARK_AS_GHOST\u0026#39; and [AllocUnitName]=\u0026#39;sys.sysobjvalues.clst\u0026#39; ",
        "permalink": "https://brianbonk.dk/blog/undelete-object-from-database/",
        "tags": [
            "t-sql"
        ],
        "title": "Undelete Object From Database"
    },
    {
        "categories": [
            "SQL Server"
        ],
        "content": "First of all, a quick recap on what a recursive query is.\nRecursive queries are useful when building hierarchies, traverse datasets and generate arbitrary rowsets etc. The recursive part (simply) means joining a rowset with itself an arbitrary number of times.\nA recursive query is defined by an anchor set (the base rowset of the recursion) and a recursive part (the operation that should be done over the previous rowset).\nThis blogpost will cover some of the basics in recursive CTE’s and explain the approach done by the SQL Server engine.\nThe basics A recursive query helps in a lot of scenarios. For instance, where a dataset is built as a parent-child relationship and the requirement is to “unfold” this dataset and show the hierarchy in a ragged format.\nA recursive CTE has a defined syntax and can be written in general terms like this …but don’t run way because of the general syntax. A lot of examples (in real code) will come:\nselect result_from_previous.* from result_from_previous union all select result_from_current.* from set_operation(result_from_previous, mytable) as result_from_current Or rewritten in another way:\nselect result_from_previous.* from result_from_previous union all select result_from_current.* from result_from_previous.* join mytable on condition(result_from_previous) Another way to write the query (using cross apply):\nselect result_from_current.* from result_from_previous cross apply ( select result_from_previous.* union all select * from mytable where condition(result_from_previous.*) ) as result_from_current The last one, with the cross apply, is row based and a lot slower than the other two. It iterates over every row from the previous result and computes the scalar condition (which returns true or false). The same row then gets compared to each row in mytable and the current row of result_from_previous. When these conditions are real – the query can be rewritten as a join. Why you should not use the cross apply for recursive queries.\nThe reverse – from join to cross apply – is not always true. To know this, we need to look at the algebra of distributivity.\nDistributivity algebra Most of us have already learned that below mathematics is true:\nX x (Y + Z) = (X x Y) + (X x Z)\nBut below is not always true:\nX ^ (Y x Z) = (X ^ Z) x (X ^ Y)\nOr said with words, distributivity means that the order of operations is not important. The multiplication can be done after the addition and the addition can be done after the multiplication. The result will be the same no matter what.\nThis arithmetic can be used to generate the relational algebra. It’s pretty straight forward:\nset_operation(A union all B, C) = set_operation(A, C) union all set_operation(B, C)\nThe condition above is true as with the first condition in the arithmetic.\nSo the union all over the operations is the same as the operations over the union all. This also implies that you cannot use operators like top, distinct, outer join (more exceptions here). The distribution is not the same between top over union all and union all over top. Microsoft has done a lot of good thinking in the recursive approach to reach one ultimate goal – forbid operators that do not distribute over union all.\nWith this information and knowledge our baseline for building a recursive CTE is now in place.\nThe first recursive query Based on the intro and the above algebra we can now begin to build our first recursive CTE.\nConsider a sample rowset (sampletree):\n   id parentId name     1 NULL Ditlev   2 NULL Claus   3 1 Jane   4 2 John   5 3 Brian    From above we can see that Brian refers to Jane who refers to Ditlev. And John refers to Claus. This is fairly easy to read from this rowset – but what if the hierarchy is more complex and unreadable?\nA sample requirement could be to “unfold” the hierarchy in a ragged hierarchy so it is directly readable.\nThe anchor We start with the anchor set (Ditlev and Claus). In this dataset the anchor is defined by parentId is null.\nThis gives us an anchor-query like below: Now on to the next part.\nThe recursive After the anchor part, we are ready to build the recursive part of the query.\nThe recursive part is actually the same query with small differences. The main select is the same as the anchor part. We need to make a self join in the select statement for the recursive part.\nBefore we dive more into the total statement – I’ll show the statement below. Then I’ll run through the details. Back to the self-reference. Notice the two red underlines in the code. The top one indicates the CTE’s name and the second line indicates the self-reference. This is joined directly in the recursive part in order to do the arithmetic logic in the statement. The join is done between the recursive results parentId and the id in the anchor result. This gives us the possibility to get the name column from the anchor statement.\nNotice that I’ve also put in another blank field in the anchor statement and added the parentName field in the recursive statement. This gives us the “human readable” output where I can find the hierarchy directly by reading from left to right.\nTo get data from the above CTE I just have to make a select statement from this: And the results: I can now directly read that Jane refers to Ditlev and Brian refers to Jane.\nBut how is this done when the SQL engine executes the query – the next part tries to explain that.\nThe SQL engines handling Given the full CTE statement above I’ll try to explain what the SQL engine does to handle this.\nThe documented semantics is as follows:\n Split the CTE into anchor and recursive parts Run the anchor member creating the first base result set (T0) Run the recursive member with Ti as an input and Ti+1 as an output Repeat step 3 until an empty result set is returned Return the result set. This is a union all set of T0 to Tn  So let me try to rewrite the above query to match this sequence.\nThe anchor statement we already know: First recursive query: Second recursive query: The n recursive query: The union all statement: This gives us the exactly same result as we saw before with the rewrite: Notice that the statement that I’ve put in above named Tn is actually empty. This to give the example of the empty statement that makes the SQL engine stop its execution in the recursive CTE.\nThis is how I would describe the SQL engines handling of a recursive CTE.\nBased on this very simple example, I guess you already can think of ways to use this in your projects and daily tasks.\nBut what about the performance and execution plan?\nPerformance The execution plan for the original recursive CTE looks like this: The top part of this execution plan is the anchor statement and the bottom part is the recursive statement.\nNotice that I haven’t made any indexes in the table, so we are reading on heaps here.\nBut what if the data is more complex in structure and depth. Let’s try to base the answer on an example:\nFrom the attached sql code you’ll find a script to generate +20.000 rows in a new table called complextree. This data is from a live solution and contains medical procedure names in a hierarchy. The data is used to show the relationships in medical procedures done by the Danish hospital system. It is both deep and complex in structure. (Sorry for the Danish letters in the data…).\nWhen we run a recursive CTE on this data – we get the exactly same execution plan: This is also what I would expect as the amount of data when read from heaps very seldom impact on the generated execution plan.\nThe query runs on my PC for 25 seconds.\nNow let me put an index in the table and let’s see the performance and execution plan.\nThe index is only put on the parentDwId as, according to our knowledge from this article is the recursive parts join column.\nThe query now runs 1 second to completion and generates this execution plan: The top line is still the anchor and the bottom part is the recursive part. Notice now the SQL engine uses the non-clustered index to perform the execution and the performance gain is noticeable.\nConclusion I hope that you’ve now become more familiar with the recursive CTE statement and are willing to try it on your own projects and tasks.\nThe basics is somewhat straight forward – but beware that the query can become complex and hard to debug as the demand for data and output becomes stronger. But don’t be scared. As I always say – “Don’t do a complex query all at once, start small and build it up as you go along”.\nHappy coding.\n",
        "permalink": "https://brianbonk.dk/blog/ready-set-go-how-does-sql-server-handle-recursive-ctes/",
        "tags": [
            "Engine"
        ],
        "title": "Ready, SET, Go – How Does SQL Server Handle Recursive CTE’s"
    },
    {
        "categories": [
            "SQL Server"
        ],
        "content": "I attended a TDWI conference in May 2016 in Chicago. Here I got a hint about the datatype hierarchyid in SQL Server which could optimize and eliminate the good old parent/child hierarchy.\nUntil then I (and several other in the class) hadn’t heard about the hierarchyid datatype in SQL Server.\nSo here’s an article covering some of the aspects of the datatype hierarchyid – including:\n Introduction How to use it How to optimize data in the table How to work with data in the hierarchy-structure Goodies  Introduction The datatype hierarchyid was introduced in SQL Server 2008. It is a variable length system datatype. The datatype can be used to represent a given element’s position in a hierarchy – e.g. an employee’s position within an organization.\nThe datatype is extremely compact. The storage is dependent in the average fanout (fanout = the number of children in all nodes). For smaller fanouts (0-7) the typical storage is about 6 x Log A * n bits. Where A is the average fanout and n in the total number of nodes in the tree. Given above formula an organization with 100,000 employees and a fanout of 6 levels will take around 38 bits – rounded to 5 bytes of total storage for the hierarchy structure.\nThough the limitation of the datatype is 892 bytes there is a lot of room for extremely complex and deep structures.\nWhen representing the values to and from the hierarchyid datatype the syntax is: [level id 1]/[level id 2]/..[level id n]\nExample: 1/7/3\nThe data between the ‘/ can be of decimal types e.g. 0.1, 2.3 etc.\nGiven two specific levels in the hierarchy a and b given that a \u0026lt; b means that b comes after a in a depth first order of comparison traversing the tree structure. Any search and comparison on the tree is done this way by the SQL engine.\nThe datatype directly supports deletions and inserts through the GetDescendant method (see later for full list of methods using this feature). This method enables generation of siblings to the right of any given node and to the left of any given node. Even between two siblings. NOTE: when inserting a new node between two siblings will produce values that are slightly less compact.\nHow to use it Given an example of data – see compete SQL script at the end of this post to generate the example used in this post. The Num field is a simple ascending counter for each level member in the hierarchy.\nThere are some basic methods to be used in order to build the hierarchy using the hierarchy datatype.\nGetRoot method The GetRoot method gives the hierarchyid of the rootnode in the hierarchy. Represented by the EmployeeId 1 in above example.\nThe code and result could look like this: The value ‘0x’ from the OrgPath field is the representation of the string ‘/’ giving the root of the hierarchy. This can be seen using a simple cast to varchar statement: Building the new structure with the hierarchyid dataype using a recursive SQL statement: Notice the building of the path after the union all. This complies to the above mentioned syntax for building the hierarchy structure to convert to a hierarchyid datatype.\nIf I was to build the path for the EmployeeId 10 (Name = ‘Mads’) in above example it would look like this: ‘/2/2/’. A select statement converting the hierarchyid field OrgPath for the same record, reveals the same thing: Notice the use of the ToString method here. Another build in method to use for the hierarchyid in SQL Server.\nGetLevel method The GetLevel method returns the current nodes level with an index of 0 from the top: GetDescendant method This method returns a new hierarchyid based on the two parameters child1 and child2.\nThe use of these parameters is described in the BOL HERE.\nBelow is showed some short examples on the usage.\nGetting a new hierarchyid when a new employee referring to top manager is hired: Getting a new hierarchyid when a new hire is referring to Jane on the hierarchy: Dynamic insert new records in the hierarchy table – this can easily be converted into a stored procedure: Notice the new GetAncestor method which takes one variable (the number of steps up the hierarchy) and returns that levels Hierarchyid. In this case just 1 step up the hierarchy.\nMore methods There are several more methods to use when working on a hierarchy table – as found on BOL:\nGetDescendant – returns a new child node of a given parent. Takes to parameters.\nGetLevel – returns the given level for a node (0 index)\nGetRoot – returns a root member\nToString – converts a hierarchyid datatype to readable string\nIsDescendantOf – returns boolean telling if a given node is a descendant of given parent\nParse – converts a string to a hierarchyid\nRead – is used implicit in the ToString method. Cannot be called by the T-SQL statement\nGetParentedValue – returns node from new root in case of moving a given node\nWrite – returns a binary representation of the hierarchyid. Cannot be called by the T-SQL statement.\nOptimization As in many other scenarios of the SQL Server the usual approach to indexing and optimization can be used.\nTo help on the usual and most used queries I would make below two indexes on the example table: But with this like with any other indexing strategy – base it on the given scenario and usage.\nGoodies So why use this feature and all the coding work that comes with it?\nWell – from my perspective – it has just become very easy to quickly get all elements either up or down from a given node in the hierarchy.\nGet all descendants from a specific node If I would like to get all elements below Jane in the hierarchy I just have to run this command: Think of the work you would have to do if this was a non hierarchy structured table using only parent/child and recursive SQL if the structure was very complex and deep.\nI know what I would choose.\nConclusion As seen above the datatype hierarchyid can be used to give order to the structure of a hierarchy in a way that is both efficient and fairly easy maintained.\nIf one should optimize the structure even further, then the EmployeeId and the ManagerId could be dropped as the EmployeeId is now as distinct as the OrgPath and can be replaced by this. The ManagerId is only used to build the structure – but this is now also given by the OrgPath.\nHappy coding…\n",
        "permalink": "https://brianbonk.dk/blog/use-of-hierarchyid-in-sql-server/",
        "tags": [
            "T-SQL"
        ],
        "title": "Use of Hierarchyid in SQL Server"
    },
    {
        "categories": [
            "SQL Server"
        ],
        "content": "Recently I got a request inside my organization to make sure that a dimension would keep track of the changes due to requrementes from the business.\nThis needed to be done in a single transaction in pure T-SQL code.\nSo – what to do and how to do it. Here’s one way.\nThe sourcetable looks like this:\nThe request was to keep track of changes in the ManagerId according to CaseId.\nI’ve created a SCD2 table like this:\nCREATE TABLE [dbo].[CaseProjectManagerHistory]( [dwid] [bigint] IDENTITY(1,1) NOT NULL, [CaseId] [int] NULL, [ManagerId] [int] NULL, [dwDateFrom] [date] NULL, [dwDateTo] [date] NULL, [dwIsCurrent] [bit] NULL, [dwChangeDate] [date] NULL ) The fields are as follows:\ndwid: Identifier for the table\nCaseId: The caseid for the rows\nManagerId: The managerid for the row\ndwDateFrom: The date from where the row is actual\ndwDateTo: The date to where the row is actual\ndwIsCurrent: Boolean that tells if the row is the current one or not\ndwChangeDate: The date of the change (if the row has changed since the first write)\nIf you need to catch up on the history types in a dimension – then take a look at Kennie’s blogppost HERE.\nFirst of all I started out with a merge statement that would insert all the new values not in the table and update the ones that needed update.\nSomething like this:\nmerge dbo.CaseProjectManagerHistory as target using (select CaseId, ManagerId, cast(getdate() as date) as startDate, datefromparts(2199,1,1) as endDate, 1 as [current], cast(getdate() as date) as changeDate from dbo.[Case]) as source on target.CaseId = source.CaseId when not matched by target then insert (CaseId, ManagerId, dwDateFrom, dwDateTo, dwIsCurrent, dwChangeDate) values (source.CaseId, source.ManagerId, source.startDate, source.endDate, source.[current], source.changeDate) when matched and target.dwIsCurrent = 1 and exists (select source.CaseId, source.ManagerId except select target.CaseId, target.ManagerId) and target.dwChangeDate \u0026amp;lt;= source.ChangeDate and source.changeDate \u0026amp;lt; target.dwDateTo then update set dwIsCurrent = 0, target.dwChangeDate = source.changeDate, target.dwDateTo = dateadd(d,-1,source.startDate) Those of you who haven’t tried and worked with a merge-statement – you can get the 101 from BOL here.\nBut this merge statement only inserts new rows and updates existing rows. The rows that are updated still needs to be in the table in order to fully apply to the SCD 2 rules.\nThis can be done by using the cluse ‘output’ from the merge-statement and then use the output rows to insert into the same table.\nIt will look like this:\ninsert into dbo.CaseProjectManagerHistory_demo (CaseId, ManagerId, dwDateFrom, dwDateTo, dwIsCurrent, dwChangeDate) select CaseId, ManagerId, startDate, endDate, [current], changeDate from ( merge dbo.CaseProjectManagerHistory_demo as target using ( select CaseId ,ManagerId ,cast(getdate() as date) as startDate ,datefromparts(2199,1,1) as endDate ,1 as [current] ,cast(getdate() as date) as changeDate from dbo.[Case] where 1=1 and caseid in (2005,2013,2015,2016,2019,2021,2023,2025,2027,2028) ) as source on target.CaseId = source.CaseId when not matched by target -- inds\u0026amp;aelig;t nye r\u0026amp;aelig;kker \tthen insert (CaseId, ManagerId, dwDateFrom, dwDateTo, dwIsCurrent, dwChangeDate) values (source.CaseId, source.ManagerId, source.startDate, source.endDate, source.[current], source.changeDate) when matched -- opdater eksisterende r\u0026amp;aelig;kker \tand target.dwIsCurrent = 1 and exists (select source.CaseId, source.ManagerId --filtrer kun p\u0026amp;aring; r\u0026amp;aelig;kker der ikke allerede eksisterer i target \texcept select target.CaseId, target.ManagerId) and target.dwChangeDate \u0026amp;lt;= source.ChangeDate and source.changeDate \u0026amp;lt; target.dwDateTo then update set dwIsCurrent = 0, target.dwChangeDate = source.changeDate, target.dwDateTo = dateadd(d,-1,source.startDate) output $action ActionOut, source.CaseId, source.ManagerId, source.startDate, source.endDate, source.changeDate, source.[current]) as mergeOutput where mergeOutput.ActionOut = \u0026#39;UPDATE\u0026#39;; The mergestatement ‘output’ action is used to insert the same rows to the history table once more. The only change is the ‘end date’.\nHappy coding!\nNote: I did a short presentation with this at my workplace a few weeks ago, and here Kennie (l, b, t) told me that there is a bug in the merge statement that needs to be taken into account. Read more of that here.\n",
        "permalink": "https://brianbonk.dk/blog/update-scd-type-2-dimension-in-one-single-transaction-using-only-t-sql/",
        "tags": [
            "T-SQL"
        ],
        "title": "Update SCD Type 2 Dimension in One Single Transaction Using Only T SQL"
    },
    {
        "categories": [
            "SQL server"
        ],
        "content": "Along with the release of SQL server 2016 CTP 3 now comes the preview of a brand new feature for on premise databases – the Query Store. This feature enables performance monitoring and troubleshooting through the log of executed queries.\nThis blogpost will cover the following aspects of the Query Store feature:\n Introduction How to activate it Configuration options What information is found in the Query Store How to use the feature What’s in it for me  Introduction The new feature Query Store enables everyone with responsibility for SQL server performance and troubleshooting with insight to the actual queries and their query-plans. It simplifies the old way of setting up tracing, logging and event handling to a standard, out of the box, feature.\nIt enables you to find causes for performance differences due to a change in query plans. It also captures historic data from queries, plans, statistics (runtime), and keeps these for later review. This storage is divided into configured time-slots.\nAll in all, this feature enables you to monitor, capture and analyze performance issues in the server with a few standard settings.\nHow to activate it The feature can be enabled in to ways – from SSMS with mouse-clicks or from T-SQL statements.\nEnable Query Store from Management Studio From the Object Explorer pane, right-click the database and select the Properties option.\nClick the Query Store tab and change the ‘Enable’ to TRUE: Enable Query Store from T-SQL statement In a new query window, the following statement enables the Query Store feature on the database ‘QueryStoreDB’:\nALTER DATABASE QueryStoreDB SET QUERY_STORE = ON; Configuration options The Query Store has a series of configuration options. All of them can be set from SQL Server Management Studio through th GUI or using T-SQL statements.\nOPERATION_MODE – This can be READ_WRITE or READ_ONLY and states if the Query Store is to collect new data (READ_WRITE) or not to collect data and just hold current data (READ_ONLY).\nCLEANUP_POLICY – Specifies through the STALE_QUERY_THRESHOLD_DAYS the number of days for the query store to retain data.\nDATA_FLUSH_INTERVAL_SECONDS – Gives the interval in which the data written to the Query Store is persisted to the disk. The frequency, which is asynchronous, for which the transfer occurs is configured via DATA_FLUSH_INTERVAL_SECONDS.\nMAX_STORAGE_SIZE_MB – This gives the maximum size of the total data in the Query Store. If and when the limit is reached, the OPERATION_MODE is automatic changed to READ_ONLY and no more data is collected.\nINTERVAL_LENGTH_MINUTES – Gives the interval at which the data from runtime execution stats is aggregated. The option gives the fixed time window for this aggregation.\nSIZE_BASED_CLEANUP_MODE – When the data in the Query Store gets close to the configured number in MAX_STORAGE_SIZE_MB this option can control the automatic cleanup process.\nQUERY_CAPTURE_MODE – Gives the Query Store option to capture all queries or relevant queries based on execution count and resource usage.\nMAX_PLANS_PER_QUERY – The maximum number of execution plans maintained for queries.\nFrom SQL Server Management Studio, the window look like below when the Query Store is enabled. Also in the bottom of this window, you can see the current disk usage: The T-SQL syntax for setting the Query Store options is as follows:\nALTER DATABASE \u0026lt;database name\u0026gt; SET QUERY_STORE ( OPERATION_MODE = READ_WRITE, CLEANUP_POLICY = (STALE_QUERY_THRESHOLD_DAYS = 30), DATA_FLUSH_INTERVAL_SECONDS = 3000, MAX_STORAGE_SIZE_MB = 500, INTERVAL_LENGTH_MINUTES = 15, SIZE_BASED_CLEANUP_MODE = AUTO, QUERY_CAPTURE_MODE = AUTO MAX_PLANS_PER_QUERY = 1000 ); What information can be found in the Query Store Specific queries in the SQL server normally has evolving execution plans over time. Thisis due to e.g. schema changes, changes in statistics, indexes etc. Also the plan cache evicts execution plans due to a memory pressure. The result is that the query performance troubleshooting can be non-trivial and time consuming to resolve.\nThe Query Store retains multiple execution plans per query. Therefore, it can be used to enforce certain execution plans to specific queries. This is called plan forcing (see below for stored procedure to do this).\nPrior to SQL 2016 the hint ‘USE PLAN’ was used, but now it is a fairly easy task to enforce a specific execution plan to the query processor.\nMore scenarios for using the Query Store:\n Find and fix queries that have a regression in performance due to plan changes Overview of how often and in which context a query has been executed, helping the DBA on performance tuning tasks Overview of the historic plan changes for a given query Identity top n queries (by time, CPU time, IO, etc.) in the past x hours Analyze the use of resources (IO, CPU and memory)  The Query Store contains two stores – a plan store and a runtime stats store. The Plan Store persists the execution plan information and the Runtime Stats Store persists the execution statistics information. Information is written to the two stores asynchronously to optimize performance.\nThe space used to hold the runtime execution information can grow over time, so the data is aggregated over a fixed time window as per setting made in the configuration.\nWhen the Query Store feature is enabled in the database, a set of system views will be ready for queries.\nsys.database_query_store_options sys.query_context_settings sys.query_store_query sys.query_store_query_text sys.query_store_plan sys.query_store_runtime_stats sys.query_store_runtime_stats_interval\nFurthermore, a series of system stored procedures can be called:\nsp_query_store_flush_db sp_query_store_reset_exec_stats sp_query_store_force_plan sp_query_store_unforce_plan sp_query_store_remove_plan sp_query_store_remove_query\nHow to use Query Store The Query Store comes with 4 standard reports as shown below: All standard reports can be modified in several ways to fit your personal needs. This is done by selection in drop-downs and point-and-click.\nThe Regressed Queries gives an overview of the top 25 most resource consuming queries in the last hour. This includes the execution plan, a time table to see when and for how long the query took to run etc.: The Overall Resource Consumption shows 4 charts as standard based on duration, execution count, CPU time and Logical reads: The Top Resource Consuming Queries report shows in the same format as Regressed Queries only non-aggregated and with more details.\nThe Tracked Queries report shows detailed data from the selected query – here you need to find and remember the query id – this can be found, among other ways, from below queries against the Query Store system views.\nThe data from the Query Store can be accessed from the above described system views. Examples of usage can be found below.\nTop 5 queries with the longest average execution time the last hour\nSELECT TOP 5 rs.avg_duration ,qt.query_sql_text ,rs.last_execution_time FROM sys.query_store_query_text AS qt RIGHT JOIN sys.query_store_query AS q ON qt.query_text_id = q.query_text_id RIGHT JOIN sys.query_store_plan AS p ON q.query_id = p.query_id RIGHT JOIN sys.query_store_runtime_stats AS rs ON p.plan_id = rs.plan_id WHERE 1=1 AND rs.last_execution_time \u0026gt; DATEADD(hour, -1, GETUTCDATE()) ORDER BY rs.avg_duration DESC; Last 10 queries executed on the server\nSELECT TOP 10 qt.query_sql_text, q.query_id, qt.query_text_id, p.plan_id, rs.last_execution_time FROM sys.query_store_query_text AS qt JOIN sys.query_store_query AS q ON qt.query_text_id = q.query_text_id JOIN sys.query_store_plan AS p ON q.query_id = p.query_id JOIN sys.query_store_runtime_stats AS rs ON p.plan_id = rs.plan_id ORDER BY rs.last_execution_time DESC; Queries with more than one execution plan\nSELECT q.query_id ,qt.query_sql_text ,p.query_plan AS plan_xml ,p.last_execution_time FROM (SELECT COUNT(*) AS count, q.query_id FROM sys.query_store_query_text AS qt JOIN sys.query_store_query AS q ON qt.query_text_id = q.query_text_id JOIN sys.query_store_plan AS p ON p.query_id = q.query_id GROUP BY q.query_id HAVING COUNT(distinct plan_id) \u0026gt; 1) AS qm JOIN sys.query_store_query AS q ON qm.query_id = q.query_id JOIN sys.query_store_plan AS p ON q.query_id = p.query_id JOIN sys.query_store_query_text qt ON qt.query_text_id = q.query_text_id ORDER BY query_id, plan_id; Source: Monitoring Performance By Using the Query Store\nWhat’s in it for me Well, I hope that the answer to this is pretty obvious to you after reading this post 🙂\nThe Query Store feature enables any person responsible for database performance to monitor, analyze and keep track of queries, execution plans and resource usage through system views or standard reports from within SQL Server Management Studio.\nConclusion This new feature is a great add-on for the DBA (or accidental DBA) that needs to keep the analytical data in a standard form, and have an availability of query statistics and troubleshooting.\nThis blogpost is based on the latest CTP of SQL Server 2016 (CTP 3.0) which can be downloaded here: SQL Server Evaluations\n",
        "permalink": "https://brianbonk.dk/blog/query-store-the-next-generation-tool-for-every-dba/",
        "tags": [
            "Debugging"
        ],
        "title": "Query Store – The Next Generation Tool for Every DBA"
    },
    {
        "categories": [
            "SQL Server"
        ],
        "content": "This blogpost will cover the aspects of the many-to-many feature from SQL Server 2016 – including:\n Prerequisites The old way The new way  This post is based on data from the AdventureWorksDW2012 database.\nPrerequisites In order to test the new many-to-many feature from SQL Server 2016 SSAS Tabular you’ll need to download the latest CTP from Microsoft – it can be found here:\nTechnet\nAlso you’ll need the Visual Studio 2015 and the add-in for Business Intelligence:\nhttps://msdn.microsoft.com/en-us/library/mt204009.aspx\nChoose the SSDT October 2015 Preview in Visual Studio for download.\nAfter a bit of waiting with the installation, you are ready to test the functionality.\nThe old way Before showing the new (and for me right way) to do the many-to-many in SSAS Tabular, let me first show you how it was done prior to SQL Server 2016 CTP 3.0.\nThanks to the two brilliant guys from SqlBI Marco Russo ([T][2],[L][3]) and Alberto Ferrari ([T][4],[L][5]) we’ve had below approach for quite a while now.\nFirst of all you need to build a bridge table with the column that links the two tables and build a model like below illustrates.\nThe m2mKey is a concatination of the SalesOrderNumber and SalesOrderLineNumber as the Tabular still does not have the ability to handle two joins at the same time.\nThen all measures that need to take the DimSalesReason into account needed to be rewritten with some DAX coding:\nSum of UnitPrice:=CALCULATE(SUM([UnitPrice]);vBridgeSalesReason) Then the output will look something like this:\nThe new way With the CTP 3.0 release and the SSDT addon for Visual Studio 2015 now this get’s as easy as 1,2,3.\nFirst of all, it is now possible to build a datamodel directly without any bridge tables like this:\nNote the highlighted area – here you can see the many-to-many relationship. This is modelled when creating the relationship in the model like this:\nRemember to select the Filter Direction to \u0026laquo; To Both Tables \u0026raquo;.\nAnd that is it!\nThe result without doing DAX formulas:\nHAPPY CODING 🙂\n",
        "permalink": "https://brianbonk.dk/blog/many-to-many-in-ssas-tabular/",
        "tags": [
            "Tabular"
        ],
        "title": "Many to Many in SSAS Tabular"
    },
    {
        "categories": [
            "SQL server"
        ],
        "content": "With the release of SQL Server 2016 also comes a great new feature to get a live view of the current execution plan for an active query.\nThis blogpost will cover the aspects of this new feature including:\n Introduction How to activate How to use and read the output Downsides – if any  Introduction The introduction of live query plans are in the current release of SQL Server 2016 CTP 2.2 a new feature from Microsoft, which hopefully will be in the final release.\nThe feature provides real-time insights to the SQL Server engine’s query execution process. This in a visual matter when data flows from one plan operator to the next in the execution. The display will cover the usual elements of an execution plan – this including the number of rows handled, the time spend, progress of the single operators and other well-known statistics of a query execution.\nOne of the good news in this feature is the ability to show and analyze the query even before it has finished. This is good when debugging complex queries – the operators are shown with their individual performance, giving the DBA or other persons responsible for the database a faster view of the places to make the performance optimization.\nActivation In SQL Server Management Studio, there is a new option when right clicking the query window – “Include Live Query Statistics”:\nFor some reason, there is no keyboard shortcut to activate that functionality. Maybe this will come in the RTM release of SSMS for SQL Server 2016.\nThis function can also be activated from the top-menu in SSMS 2016 CTP 2.2: Note: this feature also works on SQL Server 2014 SP1 – as the feature relies on underlying DMV’s from this service pack.\nIf the session running the query has enabled either statistics XML (SET STATISTICS XML ON;) or statistics profile (SET STATISTICS PROFILE ON;) then the Live Query Statistics can also be reached from the activity monitor.\nThe DBA can also activate a server wide setting to enable Live Query Statistics on all sessions with the extended event query_post_execution_showplan – for more info click here\nRight click the current query in Active Expensive Queries and choose ‘Show Live Execution Plan’: The observant reader will now ask – why are the names not the same across the SSMS? Well – I don’t know actually.\nSecurity A database level SHOWPLAN is required to populate the Live Query Statistics and the server level VIEW SERVER STATE permission in order to see the live statistics.\nAnd voila! The query runs against an enlarged table from the AdventureWorksDW2012 with more than six mill. rows.\n…and with a bit more complex query (not optimal query design at all):\nThe output The output from the Live Query Statistics can be read like any other execution plan. The operators are the same and the depper statistics can be revealed like usual with hovering the cursor on the single operators: The data in these statistics will not change as the query runs – you have to move the cursor and hover again to get an updated info on the specific operators.\nThe DMV used is sys.dm_exec_query_profiles which can be queried and gives the same results in text-form as the graphic animations. But it is a lot more efficient and easier to decode the animations than the text-based results.\nThe catch What great new feature – and it also works on SQL Server 2014 SP1 as mentioned earlier. But there is a catch – as always:\n If the query are using columnstore indexes then the live window will not show If the query are using tables that are memory optimized then the live window will not show NSP (Natively Stored Procedures) are not supported  It only works on SQL Server 2014 SP 1 and onwards. But who isn’t using one of those in production now .\nConclusion The new feature Live Query Statistics are great for performance tuning of queries and the DBA that want to see the live performance of data loading in the database. The feature works like a charm and is, from my perspective, a nice feature.\nI hope this post makes a great start for you to work with the Live Query Statistics. This post is written based on the current CTP of SQL Server 2016 (CTP 2.2) which can be downloaded here.\nIf the feature is updated in later versions of SQL Server 2016, then this post will be updated accordingly.\n",
        "permalink": "https://brianbonk.dk/blog/behold-the-new-live-query-stats-in-sql-server-2016/",
        "tags": [
            "Performance"
        ],
        "title": "Behold the New Live Query Stats in SQL Server 2016"
    },
    {
        "categories": [
            "SQL Server"
        ],
        "content": "With the release of SQL Server 2016 comes many great new features. One of these is the implementation of row level security in the database engine.\nThis blogpost will cover the aspects of this new feature – including:\n Setup Best practice Performance Possible security leaks  Introduction The row level security feature was released earlier this year to Azure – following Microsoft’s cloud-first release concept.\nA past big issue with the SQL Server engine was that in only understands tables and columns. Then you had to simulate security using secured views, stored procedures or table value functions. The problem here was to make sure that there were no way to bypass them.\nWith SQL Server 2016, this is no longer an issue.\nNow the SQL Server engine handles the security policy in a central controlled area.\nSetup and best practice The Row-level security is based on a special inline table valued function. This function returns either a single row with a 1 or no rows based on the users rights to that specific row.\nLet us take an example:\nFirst of all, I’ll create a database and some users to test with:\nCREATE DATABASE RowFilter; GO USE RowFilter; GO CREATE USER userBrian WITHOUT LOGIN; CREATE USER userJames WITHOUT LOGIN; GO A table with examples and grant select to the new users:\nCREATE TABLE dbo.SalesFigures ( [userCode] NVARCHAR(10), [sales] MONEY) GO INSERT INTO dbo.SalesFigures VALUES (\u0026#39;userBrian\u0026#39;,100), (\u0026#39;userJames\u0026#39;,250), (\u0026#39;userBrian\u0026#39;,350) GO GRANT SELECT ON dbo.SalesFigures TO userBrian GRANT SELECT ON dbo.SalesFigures TO userJames GO Now we’ll add a filter predicate function as below:\nCREATE FUNCTION dbo.rowLevelPredicate (@userCode as sysname) RETURNS TABLE WITH SCHEMABINDING AS RETURN SELECT 1 AS rowLevelPredicateResult WHERE @userCode = USER_NAME(); GO This illustrates that the current user must have associated records in order to get any results. Notice that the functions does not have access to the rows itself.\nFurthermore the function can contain joins and lookup tables in the where clause – but beware of the performance hit here. Look further down this post for more info.\nThe last thing to do is to add a filter predicate to the table dbo.SalesFigures:\nCREATE SECURITY POLICY UserFilter ADD FILTER PREDICATE dbo.rowLevelPredicate(userCode) ON dbo.SalesFigures WITH (STATE = ON); GO That’s it.\nLet’s test the results with the users added before:\nEXECUTE AS USER = \u0026#39;userBrian\u0026#39;; SELECT * FROM dbo.SalesFigures; REVERT; GO This gives me 2 rows:\nEXECUTE AS USER = \u0026#39;userJames\u0026#39;; SELECT * FROM dbo.SalesFigures; REVERT; GO This gives me 1 row:\nThe execution plan shows a new filter predicate when this row level security is added: To clean up the examples.\nUSE master; DROP DATABASE RowFilter; Performance Some might ask, “what about the performance – isn’t there a performance hit in this use of functions?”\nThe short answer is “It depends”.\nIf you only use a direct filter on the table there is very little to no impact on the performance. The filter is applied directly to the table as any other filter. Compared to the old way of doing the row filter with stored procedures or table valued functions this new approach is performing better.\nIf you plan to use lookup tables or joins in the predicate function, then you must beware of the helper tables’ indexes and how fast they can deliver data to the function. If the tables are large and slow performing (without indexes etc.) then you will experience bad performance in the row filter function. But that’s just like any other lookup or join that you might do in your solutions.\nBest practices There are some best practices given from Microsoft:\nIt is highly recommended to create a separate schema for the RLS objects (predicate function and security policy). The ALTER ANY SECURITY POLICY permission is intended for highly-privileged users (such as a security policy manager). The security policy manager does not require SELECT permission on the tables they protect. Avoid type conversions in predicate functions to avoid potential runtime errors. Avoid recursion in predicate functions wherever possible to avoid performance degradation. The query optimizer will try to detect direct recursions, but is not guaranteed to find indirect recursions (i.e., where a second function calls the predicate function). Avoid using excessive table joins in predicate functions to maximize performance. Possible security leaks This new row filter context can cause information leakage using some carefully codes queries.\nAbove example can be breached with the following query:\nSELECT 1/([sales]-250) FROM dbo.SalesFigures WHERE Usercode = \u0026#39;userJames\u0026#39; This will give an error: Divide by zero error encountered.\nThis will tell the user trying to access the table, that userJames has a sale of 250. So even though the row filter prevents users from accessing data that they are not allowed, hackers can still try to determine the data in the table using above method.\nConclusion The new row level security feature has been very much a wanted feature for quite a while now, and with the function now in place, and planned to be released in the RTM version of SQL Server 2016, the DBA’s and other people working with security can use this out-of-the-box.\nI hope this post makes a great start for you if you would like to try out the row level security function. Currently the feature is awailable in the latest CTP version (2.2) – which can be downloaded here: SQL Server 2016 Community Technology Preview\n",
        "permalink": "https://brianbonk.dk/blog/row-level-security-in-sql-server-2016/",
        "tags": [
            "Security"
        ],
        "title": "Row Level Security in SQL Server 2016"
    },
    {
        "categories": [
            "SQL Server"
        ],
        "content": "One of the new features in SQL Server 2016 – and there is a lot – is the ability to stretch the on premise databases to an Azure environment.\nThis blogpost will cover some of the aspects of this – including:\n Primarily setup – how to get started Monitoring state of databases that are in ‘stretch mode’ Daily work with stretch databases Backup – what’s good to know  With the release of SQL Server 2016, the new feature called stretch database is also released. The feature lets you as a database administrator, make databases stretch (read: copy old data) to an Azure environment. The data is still able to respond to the normal queries that are used, in other way; there is no need to change the current setup for existing applications and other data-contracts to use this feature.\nSo when is the stretch database something you should consider When you only sometimes need to query the historical data The transactional data that are stored needs all historical data The size of the database tables are growing out of control (but not an issue of bad design – then you need to take other actions…) The backup times are too long in order to make the daily timeslots for maintenance If you have one or more marks on the above list, then you have a database that are candidate for stretching into Azure.\nA typical database in stretch mode are a transactional database with very large amounts of data (more than a billion rows) stored in a small number of tables.\nThe feature is applied to individual tables in the database – but a need for enabling the feature on database level is a prerequisite.\nThe limitations No free goodies without limitations.\nThere are a list of limitations to a stretch database. Two types of limitations, datatypes and features.\nThe datatypes that are not supported for stretch database is:\n filestream timestamp sql_variant XML geometry geography hierarchyid CLR user-defined types (UDTs)  The features that are not supported:\n Column Set Computed Columns Check constraints Foreign key constraints that reference the table Default constraints XML indexes Full text indexes Spatial indexes Clustered columnstore indexes Indexed views that reference the table  Therefore, it is advisable to have an agreement with your developers if you plan to use the stretch feature. It is more likely that they can code without the above lists, but if they already have implemented features, and needs to work around them, then you are not in good standing for a while.\nSecurity In order to handle and maintain the stretch feature the current user must be a member of the db_owner group and CONTROL DATABASE permissions is needed for enabling stretch on database level.\nSetup – how to get started First, get an Azure account. If you not already have one. Then…\nA small change in sp_configure is needed to get the feature ready.\nEXEC sp_configure ‘remote data archive’ , ‘1’; RECONFIGURE; Enabling the database It is a prerequisite to enable the database for stretch in order to enable its tables.\nIt is pretty straight forward – just right-click the database – choose tasks and select ‘Enable database for stretch’: Then the SQL Server asks you to sign in to your Azure environment.\nYou need to choose a set of settings for your stretch database in Azure – including:\n Location for the server Credential for the server Firewall rules  There is a summary page with all info – when complete, just hit ‘Finish’.\nNote: the current applications and data-contracts are NOT able to access the data in Azure directly. The only way to access this data is through the normal on premise database. This database then makes the call to access the Azure database or not based on the current configuration and state of migration (see below for help in the latter).\nEnabling tables for stretch As easy as the database, so is the tables.\nRight-click the table that you want to stretch – choose ‘Stretch’ and ‘Enable Stretch’.\nAs seen on the screenshot you can also here do the following tasks: Disable, Pause and Resume stretch. All 3 hopefully self-explainable.\nMonitoring the state of databases and tables in stretch mode There is released a list of Dynamic management views (DMVs) and updated to existing catalog views to help with the work of monitoring the state of stretch databases.\nThe DMV sys.dm_db_rda_migration_status shows you the current state, in batches and rows, of the data in the stretched tables. For more information, refer to MSDN: sys.dm_db_rda_migration_status.\nThe catalog views sys.databases and sys.tables now also contains information about the stretch feature on each part respectively. See more as MSDN: sys.databases and sys.tables.\nTo view the remote databases and tables for stretch data use the two new catalog views sys.remote_data_archive_databases and sys.remote_data_archive_tables.\nA big note for the current CTP 2.2 release: This release only supports the stretch data for entire tables. This meaning that an architectural decision needs to be taken to move historical data to separate tables. I will assume that the final release will contain a query based configuration in order to find and detect the historical data to be moved to the Azure environment.\nBackup and restore The backup and restore is the same as before the stretch feature. The same strategy must be taken and also the same precautions for data storage in Azure.\nOne must keep in mind that the on premise backup only happens with on premise data.\nThe restore process adds a step to the checklist when restoring a database with stretch enabled.\nUpon the end of restore a connection to the stretched database in Azure must be reestablished with the stored procedure sys.sp_reauthorize_remote_data_archive.\nWhen this SP is executed, the vertical arrow on this illustration is reestablished: Conclusion The stretch database feature is a very nice and good feature to get with the release of SQL Server 2016. It enables the DBA to handle historical data and storage capacity without having the consult the developers and/or architects of new solutions. Also current applications and solutions can be configured to use this new feature.\nThis post makes a great place to begin with the stretch feature of SQL Server 2016. Personally, I hope that the final feature has a bit more configuration to handle the historical data.\n",
        "permalink": "https://brianbonk.dk/blog/the-dbas-guide-to-stretch-database/",
        "tags": [
            "DBA"
        ],
        "title": "The DBAs Guide to Stretch Database"
    },
    {
        "categories": [
            "SQL Server"
        ],
        "content": "I could use the AdventureWorks2012 database, but I needed the clean datawarehouse tables in order to have minimum data maintennance when testing the BIML scripts.\nI could not find it, and figures out it was faster to make my own.\nSo heavily inspired by this post from Jonathan Kehayias (blog), I’ve made a script that can be used to enlarge the dbo.FactInternetSales table.\nThe script creates a new table called dbo.FactInternetSalesEnlarged and copies data from dbo.FactInternetSales into it with a randomizer. Exploding the data to a 100 times bigger table – est. 6 mio rows.\nGet the script here:\nEnlargeAdventureWorksDW2012\nHappy coding 🙂\n",
        "permalink": "https://brianbonk.dk/blog/enlarge-adventureworksdw2012/",
        "tags": [
            "t-sql"
        ],
        "title": "Enlarge AdventureWorksDW2012"
    },
    {
        "categories": [
            "Datacorner"
        ],
        "content": "This just got in: Second shot is back!\nYou can now retake your exam for free, until jan 12 2016.\nYay! Go get ’em…\nMore info here\n",
        "permalink": "https://brianbonk.dk/blog/second-shot-is-back/",
        "tags": [
            "certification"
        ],
        "title": "Second Shot Is Back"
    },
    {
        "categories": [
            "SQL Server"
        ],
        "content": "If either you are doing your SSIS by hand or using the BIML framework, you’ve came across the expressions and the expression-builder.\nThis is a helper list, with my most often used, and wich are allways forgotten when I need them, of my commonly used SSIS expressions.\nStrings Filename from fully qualified pathstring  RIGHT([FilePath],FINDSTRING(REVERSE([FilePath]),”\\”,1) – 1)  Folderpath from fully qualified pathstring  SUBSTRING([FilePath], 1, LEN([FilePath]) – FINDSTRING(REVERSE([FilePath] ), “\\” ,1 ) + 1)  Foldername from fully qualified pathstring  TOKEN[FilePath],”\\”,TOKENCOUNT([FilePath],”\\”) – 1)  This is only for SSIS2012 and onwards. The TOKEN and TOKENCOUNT expressions are not in prior versions of SQL Server\nFor prior versions of SQL Server:\n SUBSTRING([FilePath],LEN([FilePath]) – FINDSTRING(REVERSE([FilePath]),”\\”,2) + 2,(LEN([FilePath]) – FINDSTRING(REVERSE([FilePath]),”\\”,1)) – (LEN([FilePath]) – FINDSTRING(REVERSE([FilePath]),”\\”,2)) – 1)  Replace empty strings With SQL Server 2012 the new REPLACENULL function was implemented, making it alot easier to replace empty values.\n REPLACENULL([ColumnName], [replace value])  For earlier versions of SQL Server\n LEN([ColumnName]) == 0 ? [replace value] : [ColumnName] ISNULL([ColumnName]) ? [replace value] : [ColumnName]  Date and time Date from datetime If you want to remove the time element in a datetime object, you should cast it to DT_DBDATE. But because that datatype is very inconvenient to use, you should cast it back to the original datatype. That will set the time to 0:00.\n (DT_DATE)(DT_DBDATE)@[User::datetimeVariable] (DT_DATE)(DT_DBDATE)[datetimeColumn] (DT_DBTIMESTAMP)(DT_DBDATE)GETDATE()  Time from datetime If you want to remove the date element in a datetime object, you should cast it to DT_DBTIME. And optional cast it to a string.\n (DT_STR,8,1252)(DT_DBTIME)@[User::datetimeVariable] (DT_STR,8,1252)(DT_DBTIME)[datetimeColumn] (DT_STR,8,1252)(DT_DBTIME)GETDATE()  First day of the current month If you want to get the first day of the current month, you take the current datetime and deduct the current day number (minus 1). Optional you can remove the time part:\n DATEADD(“d”, -DAY(GETDATE()) + 1, GETDATE()) (DT_DBTIMESTAMP)(DT_DBDATE)DATEADD(“d”, -DAY(GETDATE()) + 1, GETDATE())  Last day of the current month If you want to get the last day of the current month, you add 1 month and deduct the current day number. Optional you can remove the time part:\n DATEADD(“d”, -DAY(GETDATE()), DATEADD(“m”, 1, GETDATE())) (DT_DBTIMESTAMP)(DT_DBDATE)DATEADD(“d”, -DAY(GETDATE()), DATEADD(“m”, 1, GETDATE()))  And if you realy want the last second of the current month 30-06-2011 23:59:59\n DATEADD(“s”, -1,DATEADD(“d”, -DAY(GETDATE()) + 1, DATEADD(“m”, 1, (DT_DBTIMESTAMP)(DT_DBDATE)GETDATE())))  Weeknumber of the month 1-june-2012 is weeknumber 23 in the year, but weeknumber 1 of the month june 2012.\n (DATEPART(“ww”,[YourDate]) – DATEPART(“ww”,DATEADD(“d”, -DAY([YourDate]) + 1, [YourDate]))) + 1  Datetime as concatenated string  (DT_STR, 4, 1252)DATEPART(“yyyy”, @[System::StartTime]) +\nRIGHT(“0” + (DT_STR, 2, 1252)DATEPART(“mm”, @[System::StartTime]), 2) +\nRIGHT(“0” + (DT_STR, 2, 1252)DATEPART(“dd”, @[System::StartTime]), 2) +\nRIGHT(“0” + (DT_STR, 2, 1252)DATEPART(“hh”, @[System::StartTime]), 2) +\nRIGHT(“0” + (DT_STR, 2, 1252)DATEPART(“mi”, @[System::StartTime]), 2) +\nRIGHT(“0” + (DT_STR, 2, 1252)DATEPART(“ss”, @[System::StartTime]), 2)  ",
        "permalink": "https://brianbonk.dk/blog/ssis-expressions-i-often-use/",
        "tags": [
            "SSIS"
        ],
        "title": "SSIS Expressions I Often Use"
    },
    {
        "categories": [
            "SQL Server"
        ],
        "content": "With the MIST application from Varigence – this is possible through the command line util that ships with the installation.\nWhen the installation of MIST has finished, you’ll find a new .exe-file in the installation folder called bimlc.exe.\nThis file is the core engine of the command line util.\nAccording to the online documentation found here – the command line util is pretty straight forward.\nThe observant reader will notice that the link points to a documentation for hadron.exe – this is the old name for the same tool. See this documentation from Varigence.\nThe syntax is\n`\u0026lt;br /\u0026gt; bimlc.exe -s=\u0026#34;\u0026lt;sourcefile\u0026gt;\u0026#34; -t=\u0026#34;\u0026lt;targetfolder\u0026gt;\u0026#34;\u0026lt;br /\u0026gt; ` I’ve made a quick demo biml-project with below code:\n`\u0026lt;br /\u0026gt; \u0026lt;# var meta_connectionstring = \u0026#34;Provider=SQLNCLI11;Server=[myServer];Initial Catalog=[myDatabase];Integrated Security=SSPI;\u0026#34;; #\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;#\u0026lt;br /\u0026gt; DataTable tables;\u0026lt;br /\u0026gt; tables = ExternalDataAccess.GetDataTable(meta_connectionstring, \u0026#34;[dbo].[storedProcedureThatGivesMeTheData]\u0026#34;);\u0026lt;br /\u0026gt; #\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;Biml xmlns=\u0026#34;http://schemas.varigence.com/biml.xsd\u0026#34;\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;Connections\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;OleDbConnection Name=\u0026#34;BIMLMETA\u0026#34; CreateInProject=\u0026#34;true\u0026#34; ConnectionString=\u0026#34;Provider=SQLNCLI11;Data Source=[myServer];Integrated Security=SSPI;Initial Catalog=[myDatabase]\u0026#34; /\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;/Connections\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;Packages\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;# foreach(DataRow table in tables.Rows) { #\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;Package Name=\u0026#34;SSIS package \u0026lt;#=table[\u0026#34;schema\u0026#34;] #\u0026gt;_\u0026lt;#=table[\u0026#34;table\u0026#34;] #\u0026gt;\u0026#34;\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;/Package\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;# } #\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;/Packages\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;/Biml\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;#@ import namespace=\u0026#34;System.Data\u0026#34; #\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;#@ import namespace=\u0026#34;System.Linq\u0026#34;#\u0026gt;\u0026lt;br /\u0026gt; ` This file is called AutoCompile.biml, and I want to put the generated files in c:\\AutoCompilePackages. Then the commandline would be:\n\u0026lt;br /\u0026gt; bimlc.exe -s=\u0026quot;c:\\AutoCompile\\AutoCompile.biml\u0026quot; -t=\u0026quot;c:\\AutoCompilePackages\u0026quot;\u0026lt;br /\u0026gt; \nWhen I then press ENTER the compiler starts up and does it’s thing:\nThe output tells us that there are 9 packages created in each of its own projects.\nI can find the compiled projects and packages in my defined output folder.\nAnd the content of each folder.\nThats it. Easy job 🙂 thanks to Varigence and their MIST product.\n",
        "permalink": "https://brianbonk.dk/blog/fully-automate-the-biml-expansion/",
        "tags": [
            "BIML"
        ],
        "title": "Fully automate the BIML expansion"
    },
    {
        "categories": [
            "SQL Server"
        ],
        "content": "On one of my most recent projects we had a custom script task that we needed to implement in the same package several times.\nThe code of the custom script is in its own biml-file in the project and referenced from the main biml-file.\nWe kept getting a mysterious error when executing the package in debug mode from Visual Studio. (No warning upon building the packages).\nAfter alot of debugging and hair pulling hours, we finally got our arms around the bug.\nThe error was that the ProjectCoreName of the new 3 script tasks in the same package had the same name.\nMy learning from this, and yes, I’m not a .NET developer, is that upon executing the custom script task, the SSIS engine must compile the code and store it for usage upon handling the data. This storage is shared with the whole package, and therefore cannot contain the same .NET project name. [Written from my personal understading – correct me if I’m wrong].\nSo for any future BIML users who wants to add the same custom script task to the BIML project – remember to add a variable to the ProjectCoreName of your biml-script.\nThis can be done fairly easy, as your biml-scripts can relate to the variables calling the biml-file. Just add the standard\n\u0026lt;#=variablename#\u0026gt;\nto he ProjectCoreName-tag of your biml-file containing the custom script.\nHappy BIML-ing…\n",
        "permalink": "https://brianbonk.dk/blog/ref-the-same-custom-script-task-in-ssis-with-biml/",
        "tags": [
            "BIML"
        ],
        "title": "Referencing the same custom script task in SSIS with BIML"
    },
    {
        "categories": [
            "SQL Server"
        ],
        "content": "You know this everybody – you need to install a whole new SQL Server on the customers Windows Server.\nAnd the customer doesn’t have the time or money to wait for you to click around the GUI and sit around and watch the progress-bar move every second.\nSo why not do this in a fully automatic way and in the same time save both effort and expensive time doing so?\nUsing the command prompt There is a command line util that gives you the ability to write a very very very long command and in that way install the SQL Server from there. But I personally do not want to sit around and write these commands every time.\nUsing config files A very more efficient and reusable way is to make your own configuration file and use this every time you need to install a new SQL Server.\nWhen you click your way through the UI to install the SQL Server the UI actually make a config file and then use this file to install using a very small commandline:\nsetup.exe /configurationfile=config.ini (if ‘config.ini’ is your config-file).\nThe configurationfile is easy to model around with and in that way you can have your own personal config-file that can be adapted to every environment and every customer. And if the customer for some reason needs to reinstall the server again, they just need the media and the config-file.\nThe upside When you use configurationfiles and the command line the installation can be done in, for my example on my laptop, in aprox. 5 minutes.\nHow long do you normally wait for the installation to finish?\nDownsides… I really can’t find any. Once you’ve got the config-file up and running, it’s easy-peasy to install a new SQL Server. You can also reuse the config-file for a newer version of SQL Server – just add or modify the file to fulfill the needs for that version and that environment.\nIf you have encountered any downsides that I haven’t seen – then please leave a comment and I’ll get back to you.\nFurther reading Microsoft has good documentation for this:\nCommand prompt installation\nConfiguration file installation\n",
        "permalink": "https://brianbonk.dk/blog/install-sql-server-in-5-mins/",
        "tags": [
            "Installation"
        ],
        "title": "Install SQL Server in 5 mins"
    },
    {
        "categories": [
            "Power BI"
        ],
        "content": "The Power Query tool has just been updated and there are some pretty neat features rolled out:\n New Transfomation: Use Headers as First Row. Column Name field in Merge Columns dialog. Quick Access Toolbar in Query Editor dialog. Warning for stale data previews in new queries. Function Invocation from Search. Preserve Excel Number formats in the worksheet on query refresh.  Along side this update – there is also an updated Salesforce connector for Power Query which you need to install after installing the above update.\nThere is a MSDN blog with more info here: MSDN\nDownloads: Power Query update\nSalesforce connector\n",
        "permalink": "https://brianbonk.dk/blog/power-query-october-2014-update/",
        "tags": [
            "M (power query)"
        ],
        "title": "Power Query – October 2014 update"
    },
    {
        "categories": [
            "SQL Server"
        ],
        "content": "On several occasions I\u0026rsquo;ve been assigned the task to split a delimited string into rows.\nI\u0026rsquo;ve done this in different ways, but never thought about the performance or stability of the different approaches for doing this.\nSo here\u0026rsquo;s my 25 cents and findings.\nMy first solution to this was to code a function to traverse through the string and insert a new value to a temp table for every delimiter found in the string:\nCREATE FUNCTION [dbo].[list_to_table] ( @list varchar(4000) ) RETURNS @tab TABLE ( item int ) BEGIN IF CHARINDEX(\u0026#39;,\u0026#39;,@list) = 0 or CHARINDEX(\u0026#39;,\u0026#39;,@list) is null BEGIN INSERT INTO @tab (item) VALUES (@list); RETURN; END DECLARE @c_pos INT; DECLARE @n_pos INT; DECLARE @l_pos INt; SET @c_pos = 0; SET @n_pos = CHARINDEX(\u0026#39;,\u0026#39;,@list,@c_pos); WHILE @n_pos \u0026lt; 0 BEGIN INSERT INTO @tab (item) VALUES (CAST(SUBSTRING(@list,@c_pos+1,@n_pos - @c_pos-1) as int)); SET @c_pos = @n_pos; SET @l_pos = @n_pos; SET @n_pos = CHARINDEX(\u0026#39;,\u0026#39;,@list,@c_pos+1); END; INSERT INTO @tab (item) VALUES (CAST(SUBSTRING(@list,@l_pos+1,4000) as int)); RETURN; END Then I ran into performance issues with very long strings - pushing me to find a better solution with more performance. I began to look into the xml-aproach for solving the issue - and ended up with this:\ndeclare @string as nvarchar(4000) select r.value(\u0026#39;@value\u0026#39;,\u0026#39;int\u0026#39;) as Kategori from ( select cast(\u0026lt;A value = \u0026#34;\u0026#39;+ replace(@string,\u0026#39;,\u0026#39;,\u0026#39;\u0026#34;/\u0026lt;\u0026gt;;A value = \u0026#34;\u0026#39;)+ \u0026#39;\u0026#34;\u0026gt;\u0026#39; as xml ) as xml_str) xml_cte cross apply xml_str.nodes(\u0026#39;/A\u0026#39;) as x(r) Performance for the two different solutions is shown below:\nString contains all numbers from 0 to 100 with comma as delimiter, machine 4 cores 16 gb ram and ssd.\nFunction: 7 ms (on average)\nXML: 3 ms (on average)\nNo matter how long a string I send to the XML code it runs around 3 ms - the function just climbs and climbs in time (naturally).\nAnybody who has done the same and found an even better way to dynamically split a string into rows and want to share?\n",
        "permalink": "https://brianbonk.dk/blog/split-delimited-string-into-rows/",
        "tags": [
            "t-sql"
        ],
        "title": "Split delimited string into rows"
    },
    {
        "categories": [
            "Datacorner"
        ],
        "content": "With a little push from some of my colleagues, I have submitted a speaksersession to SQL Saturday in Copenhagen and Edinburgh.\nTo my big surprise both sessions (with the same subject) was chosen as a part of the sessions to be held. I’m very excited and thrilled that these possibilities was given to me.\nThe SQL Saturday in Copenhagen is going to be my virgin session, where I’ll speak about a dynamic approach to partition kubes – bassed on the multidimensional model. The Tabular model’s dynamic partitioning can be read in a blogpost HERE.\nI hope to see you at the events – it’s free. You can sign up here: Copenhagen - Edinburgh.\n",
        "permalink": "https://brianbonk.dk/blog/im-speaking-at-sql-saturday-copenhagen-and-edinburgh/",
        "tags": [
            "Speaking"
        ],
        "title": "I’m speaking at SQL Saturday Copenhagen and Edinburgh"
    },
    {
        "categories": [
            "SQL Server"
        ],
        "content": "I came across a colleague of mine, who asked me if the new updatable columnstore index and ROLAP partitions in a Multidimensional cube is the new trend of fast and no-latency Business Intelligence.\nWell – here is my 25 cents.\nI’ll start with the updatable columnstore indexes. With SQL Server 2014 Microsoft introduces updatable columnstore indexes. Which in short terms defines that the columnstore no longer has to be dropped/disabled when loading data to the table. When the new data is loaded to the table, it is therefore first loaded to a temporary deltastore where background processes splits it into rowgroups, does indexing and compression.\nThis progress can be observed through the DMV sys.column_store_row_groups. Books online documents that every rowgroup can have 3 statuses: OPEN, CLOSED and COMPRESSED.\nOPEN: A rowgroup in read/write state that is accepting new records. An OPEN rowgroup is still in rowstore format and has not been compressed to columnstore format yet.\nCLOSED: A rowgroup which is filled and therefore locked for read/write. This rowgroup still needs compression\nCOMPRESSED: A rowgroup that has been locked and compressed and a part of the CS index.\nThe first two states are not yet part of the CS Index – but resides as internal objects in the database. They are not to be found in sys.objects or sys.partitions. They are there… They can be found trough the sys.system_internals_partitions view.\nThe latter one above, sys.partitions, only shows row groups that are in COMPRESED state and can therefore give wrong answers if it’s used to for example calculate space used.\nTherefore, when a query is executed against a table that contains a CS index that has rowgroups in OPEN or CLOSED state we will not get full benefit of the CS index, as the engine then needs to read from the rowgroups and not the CS index. Even worse is it when data already resides in the CS index and new data is loaded to the table. Then the engine needs to look at both the CS index and the rowgroups. There is no guarantee that the CS index contains all the data that is required for the query, so even if it is so, then a scan of the rowgroups are still executed.\nOne could wish that the background process of getting the rowgroups from state OPEN to COMPRESSED are blazing fast. But it is not. It is slow slow slow. A quick test on a virtual machine (4 cores 6 GB ram) shows that the background process of getting the rowgroups to COMPRESSED state is averaging with approx. 180.000 rows pr. second.\nNext up – the ROLAP partitions in Multidimensional I will not go through the explanation of building a ROLAP partitions as they are found in very good blogs around the BI universe.\nBut I’ll take a look at the way the SSAS engine gets the data from a ROLAP partition. Based in the dimensional relationships inside the SSAS cube the engine generates a t-sql statement to get the data.\nThis statement is not pretty and is not optimized in any way. It cannot be altered or user defined in any way. I have seen these queries in Xevents – and they do not perform very good overall.\nConclusion If the data is fully loaded every night in the layer just before the cube, and this data is more than a couple of million rows, then the combination is not a good option. It will take way to long time for the background process to get the CS index up and running, resulting in very poor performance at the end users.\nIf the updated data is small (a couple of 100.000 rows) and is only loaded at nighttime, then there can be arguments to use the combination. But then again, why not just load the data to a normal table and process the corresponding partition.\nSo – it depends. Partly on the solution build, the environment and the architecture.\nHappy coding.\n",
        "permalink": "https://brianbonk.dk/blog/multidimensional-rolap-partitions-and-updatable-columnstore-indexes/",
        "tags": [
            "Performance"
        ],
        "title": "Multidimensional ROLAP partitions and updatable columnstore indexes – the new black?"
    },
    {
        "categories": [
            "SQL Server"
        ],
        "content": "One of my longterm customers had a database-server crash a while ago. The server would not spin up after reboot. The SSISDB crash also gave problems.\nThe whole installation including tweeks and other stuff had to be re-attached to a new and fresh installation of SQL Server 2012.\nIt was s fairly young BI-server, so the damage was not that big after all.\nUntill today…\nSSISDB crash The development of SSIS project to maintain the BI sollution had to be deployed to the SSISDB catalog.\nI got an error like below:\nThe certificate, asymmetric key, or private key data is invalid. Changed database context to ‘SSISDB’. (Microsoft SQL Server, Error: 15297) Microsoft has defined the error but not yet documentet it – this link is displayed when I press the copy button in the dialog from above and paste into notepad:\nhttp://www.microsoft.com/products/ee/transform.aspx?ProdName=Microsoft+SQL+Server\u0026amp;ProdVer=11.00.2100\u0026amp;EvtSrc=MSSQLServer\u0026amp;EvtID=15297\nI found this blogpost – which guided me to the sollution. A place in his guide – he mentions this code:\nALTER SERVICE MASTER KEY FORCE REGENERATE; But that this not do the trick – I got an error telling me that I could not force regenerate the key. I tried to remove the ‘FORCE’ command and things started to happen upon execution.\nI got a ‘command completed successfully’ and thought that now I could carry on with Gilberts guide. But but – again I ran into troubles.\nI ran the above script but another error apeared. The login was allready existing on the server. But when browsing the login folder I then came to a halt. I do not know what caused this, me scripting or the guy reinstating the server from the crash. But the login as mentioned above had changed name – not much – but still enough for me not to notice at first glanse.\nThe prefix and suffix with ‘##’ were gone from the login. The login had been changed from the correct ‘[##MS_SQLEnableSystemAssemblyLoadingUser##]’ to ‘[MS_SQLEnableSystemAssemblyLoadingUser]’.\nI manually changed the login name and added the two ‘##’ in each end of the name – and voila!\nEverything is now up and running again.\nConclusion I got a good guide from Gilbert Quevauvilliers – thank you so much. I may have found out myself – but the time to come to the thought that the login had changed name somehow – that would have been the last thing I would check on the list of – well – alot of things.\nI would hope that Microsoft could be a little more explicit in their product help – but as allways it’s a hard prioritized list of things for them to do, and they cannot do it all at once.\nI’ve learned a but from this – and hope that You the reader could use this help.\n",
        "permalink": "https://brianbonk.dk/blog/ssisdb-crash-certificate-asymmetric-key-or-private-key-data-invalid/",
        "tags": [
            "ssis"
        ],
        "title": "SSISDB crash – Certificate, asymmetric key or private key data invalid"
    },
    {
        "categories": [
            "Datacorner"
        ],
        "content": "Statistically it happens some time for all of us with a website – and today was my ‘lucky day’ when I got my blog hacked.\nGot a mail from my hosting-company telling me that my WordPress blog has gotten hacked and was disabled untill I changed all access-codes and made a full-re-install of the binaries.\nLuckily I’ve got backups of everything – so nothing was lost.\nEverything is a-ok now – and I hope it does not happen again.\n",
        "permalink": "https://brianbonk.dk/blog/just-got-my-blog-hacked/",
        "tags": [
            "blog"
        ],
        "title": "Just got my blog hacked"
    },
    {
        "categories": [
            "Datacorner"
        ],
        "content": "Three days of SQL Server Deep dive at SQL Rally Nordic in Stockholm, SE. The sessions have been held and the impression will last a while – at least for me.\nDay 1 – The precon ‘Data Warehouse modelling – making the right choises’ by Davide Mauri (site|blog|twitter) and Thomas Kejser (blog|twitter) was a very good walktrough of the aspects, seen from a architectural angle, of building a data warehouse.\nWith a, for me, very sad announcement that this is Thomas Kejsers last round of SQL Server speaks you need to be quick to catch a glance of his knowledge. With new tools and scripting they showed how to bring down the ‘monkeywork’ in every project, giving spare time to use on business analysis and speaking with the end users. Also a new agile approach to dimensional modelling – still on the SQL server, not in analysis services – gave the descissionmakers possibility to change their mind regarding slowly changing dimensions and history attributes.\nThey got me hooked on the new BIML scripting (referencal link) to build SSIS packages VERY FAST based on a metadriven approach. “Build 100 SSIS packages in 3 sec”.\nDay 2 – keynote and sessions The keynote preseneted by the founders of PASS SQL Rally. Main speaker of the keynote was Jim Karkanias speaking of the new buzz-word in the comunities ‘big-data’ and demystifying its background and layers. A good approach to what big data is and what it can be used for. Next up at the keynote was Judy Meyer speaking of the Excel features regarding big data. Microsofts base app for playing with data and the different datasources around the world.\nWith a start trough Power Query we really got a good understanding of the features in Microsofts Power-x pack. A good start on the sessions with Kevin Kline (blog|twitter) and his ‘SQL Server internals and Architecture’.\nHis analogy of a pit team in a racing team knowing everything about a cumbustion engine – people in a team working with SQl Server should know how the engine works. ACID properties of transactions – Atomic – just them selfes, Consistent – the same every time, isolated, durable – all or nothing. All acid properties gives overhead and CAN slow down the transactions. A good walktrough of the different engines and components of a read action and a write action.\nFollowed by ‘Powerfull T-SQL improvements that reduce query complexity’ by Hugo Kornelis (blog|twitter).\nWindow functions put to the max performancegain. A very good view of the evolvement of queries from SQL 2000 to SQl 2012. Digging through key elements of a window function – how to use them and through them gain high performance. After a good lunch we headed on with Brent Ozars (blog|twitter) ‘How the SQL Server engine thinks’ A different approach to the traditional slideshow – Brent used the audience as a SQL Server. We all had sheets of paper with data. Brent being the end user asking us (the SQL Server) for data. A funny way to do it, and it actually worked – we all learned new things. Even though it was a level 100 session. A good session right after lunch where we all naturaly are a little touched by the digestion.\nTo twist our brains and for those who were ready to really listen, the session ‘Using your brain to beat SQL Server’ by Adam Machanic (blog|twitter) and Thomas Kejser. Mathematics on a very high level and deep SQL Server internal knowledge gives the two guys awesomeness in their work.\nParty and entertainment This evening the event had arranged a good dinner and entertainment. The entertainment was two very good guys – magicians – who did a very good job with illussions and magic tricks.\nDay 2 – more sessions Kicks off with ‘High availability of SQL Server’ with Tobiaz Koprowski (twitter).\nIt’s important to have your data wehn you need it and always when you need it. High Availability of the SQL Server can therefore be important to implement. Tobiaz got around the subject in a good and practical manner. His knowledge and knowhow is high which shines through his presentation, I got enlightened.\nDavide Mauri had a another session ‘Automate DWH patterns’ – a deeper dive into the pre-con subject about BIML scripting and metadriven DWH Development.\nI’m hooked on it – and definately BIML is the next focus for me.\nThe last session for this event for me was ‘Analytical hierarchies in Cubes’ – a very good one indeed.\nInstead of having alot of measures based on time (eg: YTD, Last month, Last week etc) it is simply possible to make a dynamic calculation hierarchy based on the desired calculation. After implementation, the user can now choose what measure and calculation to use from a hierarchy instead of pulling all the possible measures into the pivot table. the user experience is also alot better, as the list of measures are alot smaller. I’ll have to look into that also.\nConclusion A very inspirational event. Even though some of the sessions could have been on a higher level from my perspective.\nI didn’t see much (read: none) of Stockholm as the event was held inside Arlanda Airport – but Again, it’s not about culture on these events, it’s about learning and bringing back new knowledge and share this knowledge with the collegues.\nLooking forward to the next event – where ever that will be.\n",
        "permalink": "https://brianbonk.dk/blog/sql-rally-nordic-2013/",
        "tags": [
            "learning"
        ],
        "title": "SQL Rally Nordic 2013"
    },
    {
        "categories": [
            "SQL Server"
        ],
        "content": "In every project on Business Intelligence there comes a time when the code needs to be deplyed to the production environment. No more development, no more manual work. It is time for dynamic partitioning.\nBut what about the partitions on the tabular cube? Do we really need to tell and learn the DBA how to handle that on a periodic plan?\nThe answer is simple: No!\nThanks to the XMLA language, the DMV’s for SSAS instances (both tabular og multidimensionel) and SSIS we can do the partitioning dynamic based on the current data in the datawarehouse.\nIn below examble I’ve made partitions for every month, but as always you need to take the current architecture, dataflow, data deliveries etc into account when creating your sollution.\nHere we go:\nWe need a table in order to keep track on the partitions and their metadata. Also a table to hold data from existing partitions in the Tabular cube:\nIn SSIS I’ve created a dataflow from the SSAS Tabular instance with the list of partitions on the table I want to partition to the table ExitingPartitionsList. We can do this thanks to the DMV’s for SSAS which also works for Tabular instances – see this link for further information.\nThe SQL-statement to get list of partitions (reference to msdn):\nThe tableID is found in the tables properties (right-click and choose ‘Properties’): The controlflow looks like this. I hope it is somewhat self-explainable: Now we need to take a look at the naming convention of the partitions. There is a need for the partition-name to have a suffix that tells the span of the partition. I’ve choosen [Tablename]-[fromdate]-[todate]. And in order to get usable data from the previous dataflow, I’ve made a view as below:\nGiving this output:\nMy facttable (Måleraflæsninger) has a field that is used to filter the partition. In this case it is ‘datekey’. In order to get all possible partitions that are needed for the project I’ve made this view:\nTake notice that I’ve made the dates end at last day of the month. This is going to be used to generate the view for the partitions later on. Based on these two views we can now generate the list of partitions that needs to be created in the Tabular project:\nNow I need to generate a set of XMLA’s. One to create a partition and one to process a partition. The easiest way to get these is to script them from the GUI in the SSAS Tabular instance.\n1: Right-click the table that I’m working with and selecting ‘Partitions…’\n2: Click ‘New partition’\n3: Here specify the name – remember the convention – and the SQL statement. Here it is important to remember the filter criteria for the partitions. In this case it is one partition for every month.\nFinally click the Script buttom and ‘Script Action to New Query Window’  Result – the areas I’ve highlighted are the ones that we need to parameterize in SSIS – more to come on that part in a bit.\nThe same way I’ve made a XMLA script to process the partition – the highlighted area is, again, to be parameterized later:\nNow we need to go to SSIS and make the logic and steps to accuire the dynamic partitioning.\nFirst of all we need to make sure that all partitions that should be in the model also exists, and if they do not exists, we’øll create them.\nThe SSIS project now needs some variables as listed below:\nThe scope varies from project to project.\nAfter all the variables has been defined, we need to make the two XMLA-variables as Expressions.\nIn the Expression builder add below codes to the respective variable:\nCreatePartitionXMLA – replace the meta-data with the one that matches your sollution:\nProcessPartitionXMLA – replace the meta-data with the one that matches your sollution:\nNow we are all set and just need to build the package.\nThe focus is now on a container like this:\nThe steps are to get all the missing partitions from our defined view and loop the result with both Create partition, Process partition and Insert data to PartitionLog.\nStep 1:\nDefine a SQL task with the following statement:\nDefine the output to ‘Full resultset’ and map the result to the variable MissingPartitions.\nAdd a ForeahLoopContainer and define the container to reference the MissingPartitions object as a ADO source and map the data to the variables as below:\nInside this container add two Analysis Services Execetute DDL Tasks and define a Analysis Services Connectione that matches the environment.\nThe first DDL (Create Partition) is defined like this:\nThe second DDL (Process Partition) is defined like this:\nThe last step is to add a record in the PartitionLog table – add a SQL task with this statement:\nAnd map the parameters like this:\nNow, when the package runs, it will get a dataset of missing partitions, loop the dataset and create and process the partitions dynamically. At the end it creates a record in the partitionlog to keep track of this.\nThe last thing we need to do is to add a container to process the latest partition every time the package executes.\nWe need to build this:\nAgain, add a SQL task and define it to get data from the view with current partition we created earlier:\nMap the ‘Full resulset’ to the variable PartitionName.\nThe foreach loop container must be defined to use this variable and the data mapped like this:\nAdd a Anaysis Services Exectute DDL task and define it to use the variable ProcesPartionXMLA. We can reuse the expression as it is defined as expression and uses the same logic in the expression.\nFinally add a SQL task with below code:\nMap the parameters like this:\nAnd there it is. All done.\nNow every time the package is executed it will see to that missing partitions is created (in this case for every month start) and processed. And it will make sure that the latest partition is updated with the latest data.\nThe processing time now takes very short time to do, as the only data that is processed is the latest one. Of course the first time the package is run it will create all the partitions and process them.\nThe whole code from above can be downloaded here:\nBlog-code Dynamic partitions\n",
        "permalink": "https://brianbonk.dk/blog/dynamic-partitioning-tabular-cube/",
        "tags": [
            "tabular model"
        ],
        "title": "Dynamic partitioning tabular cube"
    },
    {
        "categories": [
            "Datacorner"
        ],
        "content": "I got it! The title of MCSE Business Intelligence.\nMy, for now, last test was 70-467 – Designing Business Intelligence Solutions with Microsoft SQL Server 2012.\nI can now call myself MSCE.\nThe last two tests:\n 70-466 – Implementing Data Models and Reports with Microsoft SQL Server 2012 70-467 – Designing Business Intelligence Solutions with Microsoft SQL Server 2012  They were not that hard compared to the first 3 to become MSCA. There is, for now, no training kit to help you on your way. The only way is to know the ‘skills measured’, find your gaps and fill them with knowledge and practise.\nThe last test today brought case studies. 3 of them to be exact.\nA case study is a set of ‘documents’ explaining the background for a given scenario – incl. technical-, business-requrements, some architecture and a datamodel/server-infrastructure. It is followed by a set of 5-8 questions – for which you can only give the right answer if you know the background in the given scenario.\nI learned it the hard way not to read the whole text for every CS – but first read the questions and find the phrases in the text afterwords.\nSo today is a happy day – I’m done with the certification for Business Intelligence for now.\n",
        "permalink": "https://brianbonk.dk/blog/mcse-business-intelligence/",
        "tags": [
            "certification"
        ],
        "title": "Finally - MCSE Business Intelligence"
    },
    {
        "categories": [
            "Datacorner"
        ],
        "content": "Ready, Steady, GO!!\nToday I’ve just passed the last certification on the road to MCSA: SQL Server 2012. GREAT!!\nWhat a journey and what a huge pile of books, blog-links and other stuff.\nThe usual training kits from Microsoft Press has been read and been the base for further reading and study.\nA few days ago Microsoft launched their youtube channel with 3 very good videos regarding the path to MCSA: SQL Server 2012.\nIt brings one 1 hour 15 mins long video for all three certifications.\n70-461: Querying SQL Server 2012 70-462: Administering SQL Server 2012 70-463: Implementing Datawarehouse with SQL Server 2012\nBring out the notepad and a cup of your best coffee.\nMicrosofts Youtube Channel\n",
        "permalink": "https://brianbonk.dk/blog/preparations-for-certification-mcsa/",
        "tags": [
            "certification"
        ],
        "title": "Preparations for certification: MCSA – SQL Server 2012"
    },
    {
        "categories": [
            "SQL Server"
        ],
        "content": "Ever had that awesome SQL tracer build up that does just the right thing for your system to do some performance monitoring – well I know that I had. And someday you might need just the same trace again. But now you need to build it again…\nHere comes the feature Extended Events in place. It was first introduced in the SQL 2008 version. The feature is a good and lightweight event-driven mechanism for collecting information about your SQL server. The Extended Events has a lighter footprint than the old Trace Events. It also has a more programmatic approach to get the events and information that they respond to. The Extended Events has another cool feature – they are stored inside the SQL server and can be turned on and off very simple – even on a job-vise level, rather than the old-fasion way to recreate and rethink the whole semantic for every trance you need.\nAlso the Trace Events is not guaranteed to exist on future versions of SQL server – so now is the time to learn it before you are forced to do it.\nBut but, it was all T-SQL-vise to use the feature – for me that was to many arguments and variables to know them by heart. The output from the collection was XML and therefore needed more decoding to be used for optimization. So I never got to use them on a daily basis.\nAnd along came Polly the Extended Events GUI with SQL 2012.\nFinally a GUI to the Extended Events handler that ease up your daily work with the SQL server. A set of GUI interfaces have been introduced for dealing with Extended Events: a Wizard, an Extended Events Properties Editor, and a Data Viewer. The Wizard is a handy way to get walked through creating an Extended Events Session, but I’m going to skip past that and talk about the Properties Editor and the Data Viewer. These two interfaces are where you’re going to spend most of your time.\nYou can find the Extended Events Sessions under the Management folder in SQL Server Management Studio (SSMS). As a replacement (and enhancement, because it does more) for the default trace, a Session comes installed, system_health. You can use this as a great way to learn how Extended Events Sessions are set up because the Session includes many different types of Events, Targets, Filters, and Actions. The same set of windows we’re about to go over can also be used to create new Sessions in addition to edit existing ones.\nDefining a Session For illustration purposes, I’m going to stop the system_health Session while we examine its properties, just so that they’re mostly accessible. All I have to do to make this happen is right-click the session and select Stop Session from the context menu. Right-click the Session again and select Properties, and the Editor opens.\nEven though there is details in the window, most of them are self-explainable.\nIt’s a matter of supplying a Session name and, when you are creating a new Session, you can pull from a list of Session Templates.\nThe nifty feature is that you have full control of when the Session starts. When I start from scratch, I normally start the event immediately and watches the LIVE DATA in the Data Viewer window (more to come on this later).\nSelecting Events The next page down is the Events page. Clicking it on the left you’ll see all the power and flexibility of Extended Events on display.\nYup, there’s a lot of things and corners here. Let’s take them one at a time. The left side of the screen is where the main work is done – at the top there is a text field and a drop-down selection box. This lets you search the collection of events in the system. For example, in the above screenshot there’s no search in place, so all events are displayed. If I typed something in the textbox, the events below the bo would the filtered according to my search. This is a great way to quickly find the specific event you might want to use. The drop-down feature can then help you on the way by selecting e.g. ‘Event names only’, ‘Event Fields only’, ‘Event names and Description’ or ‘All’.\nOn the list itself you can further filter the Extended Events you are interested in. The headings on the columns can be sortet just by clicking on them. On the above picture the Name is sorted (the little triangle to the right of ‘Name’ indicates the sorting order. The columns Category and Channel are drop-downs that let you filter the list even further.\nThe names of the Extended Events can be very cryptic. Therefore you’ll find additional description of the selected Event at the lower left corner of the window. Right next to this, there is a list with the fields for the selected Extended Event. These fields can be carefully compared to the old fashion Columns in the Tracer; they are somewhat more inherent and unique to each Event. When capturing and Event you also by default capture most of its Fields. More on these exceptions later.\nOnce you are satisfied with your selection of Event, you click the large right arrow in the center of the screen to move that Event to the Selected Events list. Removal of items from this list is easy-peasy. Again below this list, you’ll get a second description of the Extended Event you’ve selected.\nConfiguring Events There’s even more functionality on this page. Notice that button in the top right corner that says Configure and points to the right? Click that and you get to a whole new set of functionality.\nHere you configure the Events. If you want to go back – just click the Select button.\nOn the left side of the window you have a list of Events, and some extra goodies. First the name, and here you can, again, sort as you like. Below a description of the selected Event. Right next to the name column there is a column (the one with the lightning) showing how many Global Fields, also known as Actions, have been selected. The last column shows whether the Event is filtered. This is a great way to quick and easy identify the Filters and Actions you have. In the above picture, you can see that error_reported has a filter, and none of the other Events have.\nTo the right there are 3 tabs: Global Fields (Actions), Filter (Predicate) and Event Fields. The first one – an Action, a global fieldm or an additional column that you can add to any event that you want to capture. An example is the error_reported event that is currently highlighted does not have a database_id Event Field. If you want to capture that field when an error occurs (might be a good idea), you will have to use an Action.\nThe thing is, that an Action is captured after the Event and is executed synchronously. This means that if there is anything that might cause some performance bottlenecks as a part of your Extended Event capture, here is a likely candidate (among a few others). So instead of calling them Global Fields, which can sound a little to attractive, I would prefer the name i parentheses Actions. This way it is clear that they are different and that you should use them with caution. Selecting a particular Action’s check box adds it to the Extended Event selected on the left side of the screen.\nAs shown above, clicking the Events (Predicate) tab lets you see and control which events gets captured. Also, obvious, you can add more lines in an easy-access manner. Each of the columns provides you with a drop-down list, except the final one where you’re expected to enter information. The Fields are from the event and a selection of operating system and SQL Server Fields that you can filter on. The comparison operators are the standard set of equals, less than, and so on, divided up into int and int64. The one thing I’d add is that the more immediate your first filter, the less load these will place on the system. Eliminating all errors below 20 as the first criteria is a good example.\nThe last tab shows the Event Fields, fields that are unique to this event. Don’t misunderstand me: events have lots of fields in common (such as session_id and database_id, because these are common values within SQL Server), but each event has a preselected list of Fields that apply to it.\nMostly this is just a listing of the Fields and their data types for the selected Event. However, note the event at the top of the list with the check box. By default, all Fields are included, except a few that are more expensive to collect. You have to decide whether you need these Fields when you’re setting up your Extended Events Session. Selecting the check box will include them in the Session.\nData Storage And that’s it. You’ve now seen how to select a list of Extended Events and to configure those events with Actions, Filters, and Event Fields. Up to now I haven’t talked about where all this information goes. That’s the next page. Clicking the Data Storage page on the left side of the screen via the page listing there opens up a screen like this:\nYou can define a number of different Targets for your Extended Events Sessions. It really depends on how you’re trying to consume these events. The interesting thing is, if you just want to watch the Session live, you don’t actually have to designate a target here. I don’t want to try to describe what all these are for and what they can do; there’s better documentation for that.\nBut for most situations, the likely target will be the one selected, event_file. This puts all the output into a file. When you select this Target, you get several properties that you must define at the bottom of the screen. For example, the file name and location are naturally included. You also get to decide how large you want the files to be, if you want them to rollover as they’re filled, and, if they’re rolling over, what the maximum number of files ought to be. It’s a great way to capture information like query performance metrics so that you can later load them into tables and start running reports to identify the longest running query, for example.\nAdvanced features The last page is the Advanced features\nI’ll not go deep into this except to say that here there is a lot of control over how much impact you’ll allow, or force to your setup of Extended Events. By making adjustments here, you can ensure that you have no losses to Events (and probably a much higher load on your system) or a very lossy process (with a lower impact). Look to Books Online and other resources for when and how to adjust these.\nThat’s it! Done. Click OK, and now you have a new Session (or you’ve updated an existing one) No T-SQL required. However, it is still possible to script your own Session once its created to be used on other servers if applicable.\nTime to watch SQL-telly Now I can watch queries as they go by. All I have to do is right-click the Session and select Watch Live Data, and the Data Viewer window opens as in Figure 8. What’s more, you can use the File, Open, and find *.xel files and open them directly into the same viewer.\nThe window is split in two. At the top are all the events and the timestamp for when they occurred. At the bottom are the Fields for the selected Event in the window above. You can scroll through the various data, and you’ll see everything you need, no XML required.\nScrolling around to find the data you want can be a pain, so, if you like, you can right-click a Field in the lower window and select Show Column In Table from the context menu. Below a few columns displayed in the grid.\nThere’s a bunch more functionality built into the Data Viewer. You can double-click a Field to open in a new window, which is handy for viewing long T-SQL strings or XML output. If you’re looking at either a stopped Session or a file, you can sort the grid by columns. You can’t do that while watching Live data. Best of all, and I really love this, you can toggle a bookmark on an event so that you can find your way back to that event quickly. OK, maybe that isn’t best, but it’s pretty good. You can also apply a filter based on a value in a Field to show only that one. So, for example, if I only wanted to look at the error_reported Events from above example, I could right-click that column where that is the value and select Filter On This Value from the context menu.\nIf you’re looking at a Session, not a file, you can start and stop the session, pick which window you want shown, edit your filters, and do grouping and aggregation. Actually that’s pretty slick, too. What I’ve done in Figure 11 is group by the Event name Field so that I’m seeing all of a particular event as a set.\nFrom my small run of events I have three different types: error_reported with 18 Events, rpc_completed with 4, and sql_batch_completd with 41. I’ve expanded the rpc_completed to show the individual calls. This does bring out one issue with Extended Events that I found to be a little problematic for gathering query metrics. Note that the batch_text Field is NULL for all the rpc_completed events. This is because the rpc_completed Event has a statement Field that is the equivalent to the batch_text field in the sql_batch_completed Event. A slight pain, and something to be aware of. However, you’re compensated by being able to get the object_name Field, which means you can immediately group all your stored procedure calls by the name of the procedure, no worries about trying to parse out the T-SQL information to remove parameters. That’s a huge win.\nA little extra feature is the ability search in the results as shown below.\nYou can define what field or fields you want to search through. In my case, I chose Table Columns. You can put different criteria in and even make use of wildcards and regular expressions. I left that off in my case so that I could find the literal string ‘SELECT *’.\nThere’s still more that I haven’t covered, but you get the idea. With SQL Server 2012 you get the ability to do fully fledged data exploration through your Extended Event output.\nSummary of performance monitoring There’s much more to learn about Extended Events, and you’re probably still going to use a lot of T-SQL code when working with them. But to get started, you now have a GUI that has everything you need to set up, control, and maintain Extended Events. You also have a GUI that allows you to consume and report on the data collected from Extended Events. Many of the reasons people had in the past for not using Extended Events should now be eliminated.\n",
        "permalink": "https://brianbonk.dk/blog/sql-2012_-performance-monitoring-the-light-and-right-way/",
        "tags": [
            "Performance"
        ],
        "title": "SQL 2012: Performance monitoring the light and right way"
    },
    {
        "categories": [
            "SQL Server"
        ],
        "content": "Ever wondered how to do dynamic picture alignment in SQL Reporting Services? I have for sure needed it often. Until now I’ve always answered no to requests for this.\nThe solution can be used when creating reports in SQL Server Reporting Services with dynamic pictures in different sizes based on variables inside the report. In this article the pictures will be aligned right and these are stored in a FileTable on a SQL 2012 instance – you can also do this with other types of picture storage in Reporting Services.\nWell here it goes:\nFor starters you’ll need a parameter to hold the pictures variable as follows – the default values etc. you need to define to with your own data:\n \nNext add the relevant pictures either as a datasource (this example) or loaded directly to the reports image folder. Remember to map your parameter to the query.\n \nNow add a reference in the report to the system.drawing object in Windows. A note for this: If the report is going on a Sharepoint site for production, you should choose the ver 2.0 as Sharepoint (for now) only supports up to .NET 3.5.\n \nThen – and there comes the geeky parts – first you need to find the maximum width in the collection of pictures. This is to be the width of the picture placeholder. In this case I’ve set it to 190 pixels. Place the placeholder with the right border where you want the picture to be aligned.\nThe picture inside the placeholder is here defined by a server storage as descriped earlier. The variable is used directly in the datasource.\n \nIn the placeholder for the pictures properties you’ll find the padding area – in this example we’ll need to define an expression for the left padding. (we are moving the picure from left to right in order to align it right).\nThe expression calls the system.drawing object added to the report with this content:\n=CSTR(round(190-(System.Drawing.Image.FromStream(new System.IO.MemoryStream(CType(First(Fields!file_stream.Value, \u0026quot;Kommunelogo\u0026quot;),Byte()))).Width*0.75),0)-1) + \u0026quot;pt\u0026quot;  It might look hairpulling, but do not worry – it’s not that hard to decode.\nFirst of all we need to make sure that the expessions results is a string (CStr), next the number for the padding needs to be with zerro decimals (round).\nThen I must find the missing pixels rom the maximum width of 6,2632cm or 190 pixels – therefore ‘190-‘\nWe call the system.drawing with this: System.Drawing.Image.FromStream(new System.IO.MemoryStream(CType(First(Fields!file_stream.Value, “Kommunelogo”),Byte()))).Width\nThe first bit of this should be out of the box for old .NET developers – for all us other people: accept the thing as it is or read the documentation. The interesting thing there is the bit ‘Byte().Witdh’ which tells the command to return the width of the picture. The fast readers allready now knows how to change this to return the height…\nThe output of the ‘Byte().Width’ is the pictures width in pixels. This gives us the result ‘190-picture_width’. This number then needs to be converted to points which is needed for the indent. 1px = 0,75pt.\nJust to be sure that I do not push the pictures right border out of the placeholders right border I extract 1 point.\nThe end is just to be sure that I pass the indent in the right syntax ‘”pt”‘.\nAnd that’s it!!\nNow no matter how wide the picture is, the expression in the indent controls the picture to be aligned to the right in the placeholder.\nYou can also use this feature to center your images in the placeholder. That’s pure mathematical changes to the above – I’ll let you sort that one out for your selfes :).\n",
        "permalink": "https://brianbonk.dk/blog/dynamic-picture-alignment-in-sql-reporting-services/",
        "tags": [
            "SSRS"
        ],
        "title": "Dynamic picture alignment in SQL Reporting Services"
    },
    {
        "categories": [
            "SQL Server"
        ],
        "content": "You’ve all been there, you’ve all banged your head against the monitor just because you could not remember that code for the specific date formatting you needed in SQL. I know I have.\nI SQL 2012 this is no longer such a headache to remember all those codes – JAY!\nEx. from SQL 2008 R2 and older:\nSELECT CONVERT(VARCHAR(10), GETDATE(), 105) AS [DD-MM-YYYY] --Italian  SELECT CONVERT(VARCHAR(10), GETDATE(), 104) AS [DD.MM.YYYY] --German  SELECT CONVERT(VARCHAR(10), GETDATE(), 101) AS [MM/DD/YYYY] --USA And with the codes specific for each country format needed.\nNow in SQL 2012 we are all free to try to memorize those codes for each country – all you need now is the countrys letters and language. For those of you who are familiar with Reporting Services ‘locale’ setup, this is easy-peasy.\nThere the same formats in SQL 2012:\nSELECT FORMAT(getdate(), N\u0026#39;d\u0026#39;, N\u0026#39;lt-lt\u0026#39;), --Italian  SELECT FORMAT(getdate(), N\u0026#39;d\u0026#39;, N\u0026#39;de-de\u0026#39;), --German  SELECT FORMAT(getdate(), N\u0026#39;d\u0026#39;, N\u0026#39;en-us\u0026#39;) --USA I know that my everyday coding just got a whole lot smoother and easier.\nThe complete documentation from Microsoft is here.\n",
        "permalink": "https://brianbonk.dk/blog/will-not-get-stuck-in-date-formatting-again/",
        "tags": [
            "t-sql"
        ],
        "title": "Will not get stuck in date formatting again"
    },
    {
        "categories": null,
        "content": "Hello there! I\u0026rsquo;m Brian. Technology Evangelist loving data and business intelligence. I\u0026rsquo;ve been working with data for the past 2 decades and I tend to see my work as my hobby – almost like a soccerplayer, just without all the injuries. Some people would call me geeky, I know I do😊.\nI find myself in the breaking point between tech and business and try to help organizations to achieve the most out of their investments.\nI love playing around with new stuff from Microsoft Azure and are also trying to keep an eye out for the business value.\nI\u0026rsquo;m very honored to be given the certification FTRSA - Power BI (the first and only one in Denmark in 2022).\nA bunch of Microsoft certifications on Data and Analytics - incl. MCSA and MCSE levels, CBIP from The Datawarehouse Institute in Data Analytics and Design and Leadership and Management Enterprise Architecture within TOGAF and project management from PRINCE2 is also in the books.\nAs a Microsoft Certified Trainer, I\u0026rsquo;m also fond of teaching and speaking.\nYou can see my latest and upcoming speaks and training sessions in the section Speaks.\nI hope to see you around - either at the communities or on social media. Feel free to link up on LinkeIn.\n// Follow me here:      ",
        "permalink": "https://brianbonk.dk/brian/",
        "tags": null,
        "title": "About Brian Bønk"
    },
    {
        "categories": null,
        "content": "Nothing on this page will be visible. This file exists solely to respond to /search URL.\nSetting a very low sitemap priority will tell search engines this is not important content.\n",
        "permalink": "https://brianbonk.dk/search/",
        "tags": null,
        "title": "Search Results"
    },
    {
        "categories": null,
        "content": "Below you\u0026rsquo;ll find my past and future speaks and trainings, some with links to videos, slides and other content. You can always find my speaker profile at Sessionize.\nFuture speaks and trainings September 2nd, 2022: Data:Scotland 2022 (Glasgow, Scotland, United Kingdom).Serverless, Classic or Lakehouse – The battle of architectures.\nJune, 2022 SQL Server T-SQL performance training for Fellowmind NL (Netherlands)T-SQL Performance training (10 participants)\nPast Speaks and trainings May 16th, 2022: MsBIP #70 (Copenhagen, Denmark).Azure Synapse Serverless SQL GitHub\nMay, 2022 DB-203 training for Fellowmind SE (Malmö, Sweden)DP-203 training (10 participants)\nMarch 15th, 2022: MsBIP #68 (Aarhus, Denmark).Azure Synapse Serverless SQL GitHub\nAugust 17th, 2015 MsBIP #29 (Copenhagen, Denmark)SSIS performance tuning with BIML\nMarch 29th, 2014 SQL Saturday #275 (Copenhagen, Denmark)Dynamic processing of SSAS cubes\nJune 14th, 2014 SQL Saturday #281 (Edinburgh, Scotland)Dynamic processing of SSAS cubes\n",
        "permalink": "https://brianbonk.dk/speaks/",
        "tags": null,
        "title": "Speaks"
    }
]
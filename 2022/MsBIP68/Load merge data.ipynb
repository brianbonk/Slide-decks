{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Load merge data from Data Lake\n",
        "\n",
        "Get raw data from Data Lake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-07T06:50:13.5347341Z",
              "execution_start_time": "2022-02-07T06:50:13.5344776Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-07T06:50:13.1035903Z",
              "session_id": 6,
              "session_start_time": null,
              "spark_pool": "SmallSparkpool",
              "state": "finished",
              "statement_id": 7
            },
            "text/plain": [
              "StatementMeta(SmallSparkpool, 6, 7, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "objectName = 'Patient'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "collapsed": false,
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-07T07:07:17.1614454Z",
              "execution_start_time": "2022-02-07T07:07:16.1347733Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-07T07:07:15.7166512Z",
              "session_id": 6,
              "session_start_time": null,
              "spark_pool": "SmallSparkpool",
              "state": "finished",
              "statement_id": 19
            },
            "text/plain": [
              "StatementMeta(SmallSparkpool, 6, 19, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "df = (spark\n",
        "    .read\n",
        "    .format('csv') \n",
        "    .option(\"delimiter\", \";\")\n",
        "    .option(\"multiline\", True)\n",
        "    .option(\"quote\", \"\\\"\")\n",
        "    .option(\"escape\", \"\\\"\")\n",
        "    .option(\"header\",True)\n",
        "    .option('path',f'abfss://<your own filesystem>@<your own filesystem>.dfs.core.windows.net/{objectName}')\n",
        "    .load()\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Handle schema\n",
        "Remove training header names from dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-07T07:07:22.3367299Z",
              "execution_start_time": "2022-02-07T07:07:21.856189Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-07T07:07:21.4700728Z",
              "session_id": 6,
              "session_start_time": null,
              "spark_pool": "SmallSparkpool",
              "state": "finished",
              "statement_id": 20
            },
            "text/plain": [
              "StatementMeta(SmallSparkpool, 6, 20, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df = df.select([col(c).alias(c[:c.index(' ')]) for c in df.columns])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Remove columns that are not used - if they exist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-07T07:07:26.6153336Z",
              "execution_start_time": "2022-02-07T07:07:25.571864Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-07T07:07:25.1719888Z",
              "session_id": 6,
              "session_start_time": null,
              "spark_pool": "SmallSparkpool",
              "state": "finished",
              "statement_id": 21
            },
            "text/plain": [
              "StatementMeta(SmallSparkpool, 6, 21, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ColumnnamesToDelete = spark.createDataFrame(\n",
        "    [\n",
        "        (1, \"booking_description\"),\n",
        "        (2, \"last_change_date\"),\n",
        "        (3, \"color_code_fk\"),\n",
        "        (4, \"username\"),\n",
        "        (5, \"password\"),\n",
        "        (6, \"phone\"),\n",
        "        (7, \"email\"),\n",
        "        (8, \"sms\"),\n",
        "        (9, \"hold_email\"),\n",
        "        (10, \"team_sms\")\n",
        "    ]\n",
        "    ,[\"number\",\"columnname\"]\n",
        "    )\n",
        "\n",
        "for d in ColumnnamesToDelete.collect():\n",
        "    if d[\"columnname\"] in df.columns:\n",
        "        df = df.drop(d[\"columnname\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Change column datatypes if they exits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-07T07:07:35.1219859Z",
              "execution_start_time": "2022-02-07T07:07:34.7014205Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-07T07:07:34.3107294Z",
              "session_id": 6,
              "session_start_time": null,
              "spark_pool": "SmallSparkpool",
              "state": "finished",
              "statement_id": 23
            },
            "text/plain": [
              "StatementMeta(SmallSparkpool, 6, 23, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pyspark.sql.types import StringType,BooleanType,DateType\n",
        "\n",
        "if 'date' in df.columns:\n",
        "    df = df.withColumn(\"date\", df[\"date\"].cast(DateType()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Remove characters not used in data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-07T07:07:39.505197Z",
              "execution_start_time": "2022-02-07T07:07:39.0885414Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-07T07:07:38.715116Z",
              "session_id": 6,
              "session_start_time": null,
              "spark_pool": "SmallSparkpool",
              "state": "finished",
              "statement_id": 24
            },
            "text/plain": [
              "StatementMeta(SmallSparkpool, 6, 24, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pyspark.sql.functions import regexp_replace\n",
        "\n",
        "SearchInColumns = spark.createDataFrame(\n",
        "    [\n",
        "        (1, \"start_time\"),\n",
        "        (2, \"end_time\")\n",
        "    ]\n",
        "    ,[\"number\",\"columnname\"]\n",
        "    )\n",
        "\n",
        "CharacterChanges = spark.createDataFrame(\n",
        "    [\n",
        "        (1, \"\\\\+\", \"\")\n",
        "    ]\n",
        "    ,[\"number\",\"RegExString\",\"NewString\"]\n",
        "    )\n",
        "\n",
        "for s in SearchInColumns.collect():\n",
        "    if s[\"columnname\"] in df.columns:\n",
        "        for r in CharacterChanges.collect():\n",
        "            df = df.withColumn(s[\"columnname\"], regexp_replace(col(s[\"columnname\"]), r[\"RegExString\"], r[\"NewString\"]))\n",
        "\n",
        "#display(df.limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Write data to Bronze area\n",
        "\n",
        "Write new data to the delta lake in Bronze version<br>\n",
        "Notice the APPEND part of the 4th line - this appends all data to the existing data (delta lake)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "bronze_loc = f'abfss://<your own filesystem>@<your own filesystem>.dfs.core.windows.net/Bronze/{objectName}'\n",
        "\n",
        "df.write.mode(\"append\").format(\"delta\").save(bronze_loc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Write data to Silver area\n",
        "\n",
        "Load to Silver data lake with correct partition approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-06T12:53:52.1528369Z",
              "execution_start_time": "2022-02-06T12:53:48.0522851Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-06T12:53:47.6428477Z",
              "session_id": 4,
              "session_start_time": null,
              "spark_pool": "SmallSparkpool",
              "state": "finished",
              "statement_id": 140
            },
            "text/plain": [
              "StatementMeta(SmallSparkpool, 4, 140, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# silver_loc = f'abfss://<your own filesystem>@<your own filesystem>.dfs.core.windows.net/Silver/{objectName}'\n",
        "\n",
        "# parquetFile = spark.read.parquet(bronze_loc)\n",
        "\n",
        "# parquetFile.repartition(1).write.parquet(silver_loc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Things and notes to keep for later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-01T12:47:08.6035854Z",
              "execution_start_time": "2022-02-01T12:46:46.0449079Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-01T12:46:45.6143404Z",
              "session_id": 1,
              "session_start_time": null,
              "spark_pool": "SmallSparkpool",
              "state": "finished",
              "statement_id": 55
            },
            "text/plain": [
              "StatementMeta(SmallSparkpool, 1, 55, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## managed table - hard link between objects\n",
        "## df2.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Bookings\")\n",
        "\n",
        "## unmanaged table - soft link between objects\n",
        "## df2.write.mode(\"overwrite\").format(\"delta\").option(\"path\",save_loc).saveAsTable(\"Bookings\")\n",
        "\n",
        "## statement = 'select * from Bookings'\n",
        "\n",
        "## spark.sql(statement).createOrReplaceTempView(\"Temp_Bookings\")\n",
        "\n",
        "## df2 = current_file.join(df, [df.journal_pk != current_file.journal_pk], how = 'inner' )\n"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
